{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text with an RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn.model import RNN\n",
    "from rnn.helpers import time_since\n",
    "from rnn.generate import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "The file we are using is a plain text file. We turn any potential unicode characters into plain ASCII by using the `unidecode` package (which you can install via `pip` or `conda`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 4573338\n",
      "train len:  4116004\n",
      "test len:  457334\n"
     ]
    }
   ],
   "source": [
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file_path = './shakespeare.txt'\n",
    "file = unidecode.unidecode(open(file_path).read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)\n",
    "\n",
    "# we will leave the last 1/10th of text as test\n",
    "split = int(0.9*file_len)\n",
    "train_text = file[:split]\n",
    "test_text = file[split:]\n",
    "\n",
    "print('train len: ', len(train_text))\n",
    "print('test len: ', len(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and!\n",
      "Let heaven kiss earth! now let not Nature's hand\n",
      "Keep the wild flood confined! let order die!\n",
      "And let this world no longer be a stage\n",
      "To feed contention in a lingering act;\n",
      "But let one spirit of t\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "def random_chunk(text):\n",
    "    start_index = random.randint(0, len(text) - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return text[start_index:end_index]\n",
    "\n",
    "print(random_chunk(train_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make training samples out of the large string of text data, we will be splitting the text into chunks.\n",
    "\n",
    "Each chunk will be turned into a tensor, specifically a `LongTensor` (used for integer values), by looping through the characters of the string and looking up the index of each character in `all_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string), requires_grad=True).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function loads a batch of input and target tensors for training. Each sample comes from a random chunk of text. A sample input will consist of all characters *except the last*, while the target wil contain all characters *following the first*. For example: if random_chunk='abc', then input='ab' and target='bc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_random_batch(text, chunk_len, batch_size):\n",
    "    input_data = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    target = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    for i in range(batch_size):\n",
    "        start_index = random.randint(0, len(text) - chunk_len - 1)\n",
    "        end_index = start_index + chunk_len + 1\n",
    "        chunk = text[start_index:end_index]\n",
    "        input_data[i] = char_tensor(chunk[:-1])\n",
    "        target[i] = char_tensor(chunk[1:])\n",
    "    return input_data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement model\n",
    "\n",
    "Your RNN model will take as input the character for step $t_{-1}$ and output a prediction for the next character $t$. The model should consiste of three layers - a linear layer that encodes the input character into an embedded state, an RNN layer (which may itself have multiple layers) that operates on that embedded state and a hidden state, and a decoder layer that outputs the predicted character scores distribution.\n",
    "\n",
    "\n",
    "You must implement your model in the `rnn/model.py` file. You should use a `nn.Embedding` object for the encoding layer, a RNN model like `nn.RNN` or `nn.LSTM`, and a `nn.Linear` layer for the final a predicted character score decoding layer.\n",
    "\n",
    "\n",
    "**TODO:** Implement the model in RNN `rnn/model.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating\n",
    "\n",
    "To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time.\n",
    "\n",
    "\n",
    "Note that in the `evaluate` function, every time a prediction is made the outputs are divided by the \"temperature\" argument. Higher temperature values make actions more equally likely giving more \"random\" outputs. Lower temperature values (less than 1) high likelihood options contribute more. A temperature near 0 outputs only the most likely outputs.\n",
    "\n",
    "You may check different temperature values yourself, but we have provided a default which should work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(rnn, prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = rnn.init_hidden(1, device=device)\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = rnn(prime_input[p].unsqueeze(0).to(device), hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = rnn(inp.unsqueeze(0).to(device), hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_epochs = 5000\n",
    "hidden_size = 100\n",
    "n_layers = 1\n",
    "learning_rate = 0.01\n",
    "model_type = 'gru'\n",
    "print_every = 50\n",
    "plot_every = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test(rnn, inp, target):\n",
    "    with torch.no_grad():\n",
    "        hidden = rnn.init_hidden(batch_size, device=device)\n",
    "        loss = 0\n",
    "        for c in range(chunk_len):\n",
    "            output, hidden = rnn(inp[:,c], hidden)\n",
    "            loss += criterion(output.view(batch_size, -1), target[:,c])\n",
    "    \n",
    "    return loss.data.item() / chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function\n",
    "\n",
    "**TODO**: Fill in the train function. You should initialize a hidden layer representation using your RNN's `init_hidden` function, set the model gradients to zero, and loop over each time step (character) in the input tensor. For each time step compute the output of the of the RNN and compute the loss over the output and the corresponding ground truth time step in `target`. The loss should be averaged over all time steps. Lastly, call backward on the averaged loss and take an optimizer step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, input, target, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - rnn: model\n",
    "    - input: input character data tensor of shape (batch_size, chunk_len)\n",
    "    - target: target character data tensor of shape (batch_size, chunk_len)\n",
    "    - optimizer: rnn model optimizer\n",
    "    - criterion: loss function\n",
    "    \n",
    "    Returns:\n",
    "    - loss: computed loss value as python float\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    \n",
    "    ####################################\n",
    "    #          YOUR CODE HERE          #\n",
    "    ####################################\n",
    "    hidden = rnn.init_hidden(input.shape[0])\n",
    "    rnn.zero_grad()\n",
    "    for i in range(chunk_len):\n",
    "        output, hidden = rnn(input[:, i], hidden)\n",
    "        loss += criterion(output.reshape(batch_size, -1), target[:, i]) / chunk_len\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    ##########       END      ##########\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 epochs...\n",
      "[1m 53s (50 1%) train loss: 2.0919, test_loss: 2.0935]\n",
      "Whing prichou lownd wifold are ort you of makhery balfe fraind but this shatter uttin eeping to hast c \n",
      "\n",
      "[3m 33s (100 2%) train loss: 1.8748, test_loss: 1.8952]\n",
      "Whet to in the faroble not rade you sion, me,\n",
      "You come well.\n",
      "\n",
      "OTRICHE:\n",
      "What highs more to sinake in no \n",
      "\n",
      "[4m 52s (150 3%) train loss: 1.7852, test_loss: 1.8123]\n",
      "Whe'll are in your eart.\n",
      "\n",
      "HOLONWER:\n",
      "A warm thinked horger the forther I grow, stear,\n",
      "As no murther's g \n",
      "\n",
      "[6m 16s (200 4%) train loss: 1.7176, test_loss: 1.7544]\n",
      "Whime thou sing arfull.\n",
      "\n",
      "Now is your belf.\n",
      "\n",
      "OTHELLOLLE:\n",
      "Come will ever hime a brate to more els more\n",
      "W \n",
      "\n",
      "[7m 56s (250 5%) train loss: 1.7005, test_loss: 1.7383]\n",
      "Wh the plasing in of fide\n",
      "I prout the bon, a dam mistage and felled of of o, so charte,\n",
      "Was burds, tha \n",
      "\n",
      "[9m 35s (300 6%) train loss: 1.6481, test_loss: 1.6923]\n",
      "When me: and for the hath for a\n",
      "suity the true; and hearest I take his heaver so\n",
      "tress that made hall  \n",
      "\n",
      "[11m 9s (350 7%) train loss: 1.6747, test_loss: 1.6798]\n",
      "Whome wronginess have father done thee,\n",
      "To hast the wish his mysel us.\n",
      "\n",
      "LUCENTIO:\n",
      "My four good houples \n",
      "\n",
      "[12m 43s (400 8%) train loss: 1.5962, test_loss: 1.6891]\n",
      "Whand succerous his sweated the hands\n",
      "Town my the senest be be fair good; happy\n",
      "And heard me out is ve \n",
      "\n",
      "[14m 16s (450 9%) train loss: 1.5786, test_loss: 1.6976]\n",
      "When this men man, he shall:\n",
      "Did your movest give the soper to the\n",
      "son this 'gleed, be his nuse bliand \n",
      "\n",
      "[15m 50s (500 10%) train loss: 1.5901, test_loss: 1.6771]\n",
      "When at the compears,\n",
      "So being have my housing from Is him and of as\n",
      "Thou is made did with lord?\n",
      "\n",
      "ROSA \n",
      "\n",
      "[17m 27s (550 11%) train loss: 1.6012, test_loss: 1.6711]\n",
      "Where you was time I shee,\n",
      "All to sinch I have'd that can make thinking in a farth.\n",
      "\n",
      "PROTEUS:\n",
      "O to you \n",
      "\n",
      "[19m 6s (600 12%) train loss: 1.5630, test_loss: 1.6433]\n",
      "Which in a gil me,\n",
      "So beced his man, the boy speak makest to\n",
      "clain'd: O'RGEMOMEN:\n",
      "I wass in hour thou  \n",
      "\n",
      "[20m 38s (650 13%) train loss: 1.5996, test_loss: 1.6612]\n",
      "Whibly but there is you\n",
      "As do your sural with with it.\n",
      "\n",
      "OTHELLO:\n",
      "Let him raught and all be greet in th \n",
      "\n",
      "[22m 9s (700 14%) train loss: 1.5637, test_loss: 1.6779]\n",
      "When I pray\n",
      "come mine, what say.\n",
      "\n",
      "CASTARWAR:\n",
      "This requition not my lord, they such him of days\n",
      "Hatter  \n",
      "\n",
      "[23m 29s (750 15%) train loss: 1.5956, test_loss: 1.6257]\n",
      "Where the like her of as my lover.\n",
      "\n",
      "ANTONY:\n",
      "Most there with thee shall make an thee.\n",
      "\n",
      "MARK ANTONY:\n",
      "He  \n",
      "\n",
      "[24m 38s (800 16%) train loss: 1.5378, test_loss: 1.6439]\n",
      "Whacts the king so convistance this sweet,\n",
      "Your bountal thine barge.\n",
      "Be my have thy brother by Jubecon \n",
      "\n",
      "[25m 47s (850 17%) train loss: 1.5731, test_loss: 1.6574]\n",
      "Where to their may love him of obe\n",
      "That I shall wind they rast, the tuned else\n",
      "conculal cart, if dison \n",
      "\n",
      "[26m 58s (900 18%) train loss: 1.5629, test_loss: 1.6637]\n",
      "What, right of more Henly\n",
      "how not so fortune to thie have me.\n",
      "\n",
      "BASSANIO:\n",
      "and Comforch and so true.\n",
      "\n",
      "Se \n",
      "\n",
      "[28m 6s (950 19%) train loss: 1.5669, test_loss: 1.6172]\n",
      "When they do not faction of mother alssovery\n",
      "And finely; sir,\n",
      "The hath in a grands that heir bagses fo \n",
      "\n",
      "[29m 21s (1000 20%) train loss: 1.5630, test_loss: 1.6355]\n",
      "Whap'd the hath be life\n",
      "Sick in the most the though the tatchest way death;\n",
      "The noble wasted you that, \n",
      "\n",
      "[30m 29s (1050 21%) train loss: 1.5459, test_loss: 1.6712]\n",
      "What?\n",
      "\n",
      "LANNE:\n",
      "In the worth, when sight, written bed.\n",
      "\n",
      "BEATRICE:\n",
      "Well, if their stred voud\n",
      "The majestal \n",
      "\n",
      "[31m 36s (1100 22%) train loss: 1.5482, test_loss: 1.6301]\n",
      "Where how let thy sight.\n",
      "\n",
      "MARWICLES:\n",
      "Let seatheed to the forievar, one of a heard\n",
      "And thee, behold tha \n",
      "\n",
      "[32m 42s (1150 23%) train loss: 1.5718, test_loss: 1.6500]\n",
      "Wheld:\n",
      "Beat the toung as my in my persus to.\n",
      "\n",
      "KING HENRY V:\n",
      "This talls prole accomit, he not than I sl \n",
      "\n",
      "[33m 49s (1200 24%) train loss: 1.5323, test_loss: 1.6327]\n",
      "When law a love laide good and\n",
      "And shall releve! We padam, and so was going!\n",
      "\n",
      "MISTRESS PAGE:\n",
      "What, how \n",
      "\n",
      "[34m 56s (1250 25%) train loss: 1.5341, test_loss: 1.5968]\n",
      "What' a true the blame upon not true,\n",
      "Agains and offer to my father: I do alone off,\n",
      "And bear from al  \n",
      "\n",
      "[36m 3s (1300 26%) train loss: 1.5325, test_loss: 1.6243]\n",
      "When it: in our stops and hath offend\n",
      "That won this hands poor of now right me.\n",
      "\n",
      "FALSTAFF:\n",
      "Spare no to \n",
      "\n",
      "[37m 10s (1350 27%) train loss: 1.5258, test_loss: 1.6245]\n",
      "What's this women done and hath came law:\n",
      "With suffer the soul pain--\n",
      "Whose time faults in a strenger  \n",
      "\n",
      "[38m 16s (1400 28%) train loss: 1.5320, test_loss: 1.6247]\n",
      "What is closeth and worth, if I fool,\n",
      "I yet ever not you not sword when I have pacious shorter.\n",
      "\n",
      "FLORI \n",
      "\n",
      "[39m 23s (1450 28%) train loss: 1.5400, test_loss: 1.6345]\n",
      "Whab'd on my disow bear not thy round\n",
      "That that who to habdered command, this more;\n",
      "And I grates trute \n",
      "\n",
      "[40m 30s (1500 30%) train loss: 1.5585, test_loss: 1.6588]\n",
      "Which Ray the subded yet me, and the tried.\n",
      "\n",
      "FALSTAFF:\n",
      "What, was it. Come, palk your were crown,\n",
      "Is th \n",
      "\n",
      "[41m 36s (1550 31%) train loss: 1.5433, test_loss: 1.6648]\n",
      "Where master that's all love\n",
      "And Master:\n",
      "A bid so mean the prince them to dead his swear\n",
      "no fair Freat \n",
      "\n",
      "[42m 45s (1600 32%) train loss: 1.5239, test_loss: 1.6118]\n",
      "What forget none, so covers' hath best,\n",
      "The sury, and he hope in lords, fair Rog the horse\n",
      "I had forth \n",
      "\n",
      "[43m 51s (1650 33%) train loss: 1.4997, test_loss: 1.6372]\n",
      "Where my demper. Fares then may revean the\n",
      "and against the peach all them mountagation.\n",
      "\n",
      "ROMEO:\n",
      "The tr \n",
      "\n",
      "[44m 58s (1700 34%) train loss: 1.5470, test_loss: 1.6108]\n",
      "When the leased to gords, shall so your monster.\n",
      "\n",
      "CASSIUS:\n",
      "Such short in that be to consical unpoper.\n",
      " \n",
      "\n",
      "[46m 5s (1750 35%) train loss: 1.5404, test_loss: 1.6177]\n",
      "Whatthing!\n",
      "\n",
      "CAESAR:\n",
      "I shall the rain'd their gracison\n",
      "Ow to wish a fitter's gates and purpion's gep.\n",
      "\n",
      " \n",
      "\n",
      "[47m 11s (1800 36%) train loss: 1.5147, test_loss: 1.6279]\n",
      "Where I will be; I learn,\n",
      "The sing prone on the what honour lord?\n",
      "\n",
      "SIR HENRY IAGBist:\n",
      "O bad in then th \n",
      "\n",
      "[48m 24s (1850 37%) train loss: 1.5289, test_loss: 1.6082]\n",
      "Whies all such other men men?\n",
      "Look, my good my hand death to him.\n",
      "\n",
      "AEGEON:\n",
      "Let the bad bew in this duk \n",
      "\n",
      "[49m 35s (1900 38%) train loss: 1.5196, test_loss: 1.6204]\n",
      "Where the sdeep as upon a.\n",
      "\n",
      "Third Cordient of the business to has not mine\n",
      "As arge the minds with your \n",
      "\n",
      "[50m 42s (1950 39%) train loss: 1.5277, test_loss: 1.6174]\n",
      "What sit not the pry of their worse the soul\n",
      "To have you have she were to more decial stoce;\n",
      "Whence of \n",
      "\n",
      "[51m 48s (2000 40%) train loss: 1.5018, test_loss: 1.6043]\n",
      "Where our husband?\n",
      "\n",
      "LEONTES:\n",
      "I shall a discoved made of his hands.\n",
      "\n",
      "LORD:\n",
      "How foul take you, rest brot \n",
      "\n",
      "[52m 55s (2050 41%) train loss: 1.5161, test_loss: 1.6194]\n",
      "When heard of your duke, deny in a chamber.\n",
      "\n",
      "BRUTUS:\n",
      "Fair. And our place of the mabuse still to may\n",
      "re \n",
      "\n",
      "[54m 2s (2100 42%) train loss: 1.5086, test_loss: 1.6174]\n",
      "When thou art thinkings? Thee.\n",
      "\n",
      "KING HENRY VI:\n",
      "He love and blessain-on hand and the sings,\n",
      "That and on \n",
      "\n",
      "[55m 8s (2150 43%) train loss: 1.5245, test_loss: 1.6495]\n",
      "When wronging a comes too but thy charge,\n",
      "I can repear a king should have his nature\n",
      "down the mad the  \n",
      "\n",
      "[56m 32s (2200 44%) train loss: 1.5126, test_loss: 1.6240]\n",
      "When I lord should enterty presently\n",
      "need cross his father of the world and a word:\n",
      "Must needs it is m \n",
      "\n",
      "[58m 18s (2250 45%) train loss: 1.5173, test_loss: 1.6716]\n",
      "What wert the Destixting his side\n",
      "Makes me take your could flack to all what sto was demppear\n",
      "What was \n",
      "\n",
      "[60m 1s (2300 46%) train loss: 1.5411, test_loss: 1.6013]\n",
      "Where understand that you seat.\n",
      "I must be him of it should make the trust,\n",
      "Strengeter had to a mad par \n",
      "\n",
      "[61m 24s (2350 47%) train loss: 1.5310, test_loss: 1.6349]\n",
      "Whelves and them to gardian's speak this delighty.\n",
      "\n",
      "TIMON:\n",
      "How is marry too name, thou wilt would thin \n",
      "\n",
      "[62m 44s (2400 48%) train loss: 1.5043, test_loss: 1.6070]\n",
      "Which desires it.\n",
      "\n",
      "TALBOT:\n",
      "Well, owed's old have peers, and\n",
      "with you, I will be affect the be are trai \n",
      "\n",
      "[63m 57s (2450 49%) train loss: 1.5144, test_loss: 1.6370]\n",
      "Whenier, we'll, that I am us up the\n",
      "Are an own'd of me that murder thou hast here,\n",
      "Their shall hunks w \n",
      "\n",
      "[65m 9s (2500 50%) train loss: 1.5250, test_loss: 1.5925]\n",
      "Wherethee, of many accurker the genty that his\n",
      "regation-tale freelss and man of my proper:\n",
      "Now may car \n",
      "\n",
      "[66m 21s (2550 51%) train loss: 1.5372, test_loss: 1.6158]\n",
      "Whellow I do make thy breath in\n",
      "the earth, speeeds.\n",
      "\n",
      "CLAUDIO:\n",
      "Or to me to by the place not most us,\n",
      "An \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67m 34s (2600 52%) train loss: 1.5040, test_loss: 1.6187]\n",
      "Whithtanch the poster the vows in the seem'd\n",
      "When Alubson:\n",
      "Suppere all no marking: been from this spea \n",
      "\n",
      "[68m 46s (2650 53%) train loss: 1.5182, test_loss: 1.6175]\n",
      "What is say for he's so good chase\n",
      "Of the wife officer so a earth,\n",
      "Lodleman, and advance down content, \n",
      "\n",
      "[69m 58s (2700 54%) train loss: 1.5203, test_loss: 1.6306]\n",
      "What wager of Branings of the beard.\n",
      "I thank penest you have against me with thel.\n",
      "\n",
      "KING HENRY VIII:\n",
      "N \n",
      "\n",
      "[71m 10s (2750 55%) train loss: 1.5045, test_loss: 1.6133]\n",
      "Where, came and face, the hands to my\n",
      "for thy ere I am be a leave this hours.\n",
      "What no world to make I  \n",
      "\n",
      "[72m 23s (2800 56%) train loss: 1.5051, test_loss: 1.6415]\n",
      "Whosing to be me much will degen wit's proceedity-those,\n",
      "All our light for late for the grow to be for \n",
      "\n",
      "[73m 35s (2850 56%) train loss: 1.5294, test_loss: 1.6330]\n",
      "Who you, glow glaven and more?\n",
      "\n",
      "SILVIUS:\n",
      "What's the gentleman.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "Dear she can tell t \n",
      "\n",
      "[74m 47s (2900 57%) train loss: 1.5327, test_loss: 1.6112]\n",
      "Whow, shall seck me hegrt me.\n",
      "\n",
      "LESANE:\n",
      "I shall know mudies,\n",
      "Appluls speak your constafer, being bragn  \n",
      "\n",
      "[75m 58s (2950 59%) train loss: 1.4916, test_loss: 1.6255]\n",
      "Where you may thy take times,\n",
      "So still of more thee make the true and all offy\n",
      "Who may the winds, prec \n",
      "\n",
      "[77m 13s (3000 60%) train loss: 1.5181, test_loss: 1.6211]\n",
      "Which is been he dost my gives their\n",
      "And shall be a best that was diving are me;\n",
      "For if it shall palat \n",
      "\n",
      "[78m 30s (3050 61%) train loss: 1.5085, test_loss: 1.6279]\n",
      "When that asking doubble this lady in\n",
      "thou dead me of the ears to feel me with for the say\n",
      "Wellly cons \n",
      "\n",
      "[80m 1s (3100 62%) train loss: 1.5306, test_loss: 1.6126]\n",
      "Where is the truff your great gannobs.\n",
      "\n",
      "Second Gentlementless\n",
      "Creads wish'd and the breds the play thi \n",
      "\n",
      "[81m 40s (3150 63%) train loss: 1.5093, test_loss: 1.6266]\n",
      "Whath you, servand, of every at one weapore.\n",
      "\n",
      "THOT Slius hours.\n",
      "\n",
      "CORIOLANUS:\n",
      "What smoth but readent Ba \n",
      "\n",
      "[83m 23s (3200 64%) train loss: 1.5307, test_loss: 1.6212]\n",
      "What's as darest presently done.\n",
      "\n",
      "CELIA:\n",
      "I am a gare upon it so hath our put\n",
      "And chalex, speak the nam \n",
      "\n",
      "[85m 6s (3250 65%) train loss: 1.5149, test_loss: 1.5997]\n",
      "What Fortuness? I may least at their gods out\n",
      "me the hang my cast my wold such you, sweet and\n",
      "conferin \n",
      "\n",
      "[87m 22s (3300 66%) train loss: 1.5532, test_loss: 1.6604]\n",
      "What it of queen, but great fear\n",
      "That my weep up take me like a shall as\n",
      "'Bounnoral chose never report \n",
      "\n",
      "[89m 8s (3350 67%) train loss: 1.5023, test_loss: 1.5916]\n",
      "Whelvell the worthy white are have beholdects,\n",
      "You well in a out you have preseely, Tallow\n",
      "That you ne \n",
      "\n",
      "[90m 40s (3400 68%) train loss: 1.5350, test_loss: 1.6053]\n",
      "Where, and in the death; put your demise.\n",
      "\n",
      "IMOGEN:\n",
      "Nor forget him:\n",
      "I will find him to; and beggration, \n",
      "\n",
      "[92m 12s (3450 69%) train loss: 1.5240, test_loss: 1.6045]\n",
      "Whatto's all-wings out take the king;\n",
      "A heart hear this way?\n",
      "Some forbid together not which did so:\n",
      "Fo \n",
      "\n",
      "[93m 43s (3500 70%) train loss: 1.5118, test_loss: 1.6159]\n",
      "Whilles, sir; and he will be bey know be scorn'd\n",
      "With a preserved prince.\n",
      "Now you shall of madved act  \n",
      "\n",
      "[95m 15s (3550 71%) train loss: 1.5187, test_loss: 1.6052]\n",
      "Whoy, if I do do the done his more a\n",
      "die so than thou had myself and abuse in\n",
      "Chall't from my business \n",
      "\n",
      "[96m 46s (3600 72%) train loss: 1.5272, test_loss: 1.6409]\n",
      "What, man propen honest villager prince. Let have to\n",
      "bold and that here over, prove so messe\n",
      "Scay, tha \n",
      "\n",
      "[98m 20s (3650 73%) train loss: 1.5315, test_loss: 1.6121]\n",
      "Whaht say then, when the remember and as--\n",
      "'Twas love on your trumpet and three bave all coming?\n",
      "\n",
      "Poma \n",
      "\n",
      "[99m 53s (3700 74%) train loss: 1.4835, test_loss: 1.6347]\n",
      "Where's I canst in so\n",
      "new fashion, they hath be the good duticaling entrom.\n",
      "\n",
      "DIONY:\n",
      "To the hand hand p \n",
      "\n",
      "[101m 25s (3750 75%) train loss: 1.4791, test_loss: 1.6273]\n",
      "Whichmended be lay thy hearts then,\n",
      "The cares, be man, I have lodg of her mother,\n",
      "Fraged forgeewly and \n",
      "\n",
      "[102m 56s (3800 76%) train loss: 1.5105, test_loss: 1.6447]\n",
      "When be untleaved to the scarcate\n",
      "high me, and he not speak but faithers?\n",
      "\n",
      "TITUS CALIO:\n",
      "What are a has \n",
      "\n",
      "[104m 27s (3850 77%) train loss: 1.5078, test_loss: 1.6338]\n",
      "Where's they well,\n",
      "Have both laugh have Gre of little cools;\n",
      "To repacy supported yet uncle, who have;\n",
      " \n",
      "\n",
      "[105m 53s (3900 78%) train loss: 1.5182, test_loss: 1.6302]\n",
      "When the bed by the learn'd to the come.\n",
      "\n",
      "First Merchantly, mother:\n",
      "I am your presomang thus: even of  \n",
      "\n",
      "[107m 8s (3950 79%) train loss: 1.5241, test_loss: 1.5984]\n",
      "Whom I have side.\n",
      "\n",
      "First Lord:\n",
      "Here do play some day thee, tenders master coward, that\n",
      "dious spile joi \n",
      "\n",
      "[108m 27s (4000 80%) train loss: 1.5137, test_loss: 1.6280]\n",
      "When Rousian no knight more abeastiest:\n",
      "But with this uncle, my pend. This men with moneys,\n",
      "And in the \n",
      "\n",
      "[109m 37s (4050 81%) train loss: 1.5269, test_loss: 1.6058]\n",
      "What they of vilause my soul;\n",
      "Full Cassius fighting with him monate now.\n",
      "\n",
      "ORLANDO:\n",
      "Say I know thou now \n",
      "\n",
      "[110m 45s (4100 82%) train loss: 1.4948, test_loss: 1.6156]\n",
      "Why can shall not councing you ever of your\n",
      "do, the white of boy, which is all the cither\n",
      "To make the  \n",
      "\n",
      "[111m 57s (4150 83%) train loss: 1.5363, test_loss: 1.6391]\n",
      "Whave eldess, sore the commstafule;\n",
      "And I were hell, his greet him had a sorrow: of an,\n",
      "And from thy p \n",
      "\n",
      "[113m 9s (4200 84%) train loss: 1.5388, test_loss: 1.6402]\n",
      "Whe he darry,--I will well a\n",
      "son used I leuds thou harm: then he down bear, how, I would\n",
      "such have the \n",
      "\n",
      "[114m 17s (4250 85%) train loss: 1.5119, test_loss: 1.6127]\n",
      "When of them with change desire it\n",
      "That being a book, this gold.\n",
      "\n",
      "PRINCE HENRY:\n",
      "Nor fruit us.\n",
      "So is ma \n",
      "\n",
      "[115m 25s (4300 86%) train loss: 1.5280, test_loss: 1.6150]\n",
      "Whavar? when I passion entraises;\n",
      "Gentle you may never she cursed.\n",
      "\n",
      "LAUCHESSA HENRY:\n",
      "Nature alone need \n",
      "\n",
      "[116m 33s (4350 87%) train loss: 1.4943, test_loss: 1.6385]\n",
      "When Barised borne away not raster;\n",
      "And to leave this father's just indeed, if they.\n",
      "\n",
      "KING HENRY VIII: \n",
      "\n",
      "[117m 41s (4400 88%) train loss: 1.5122, test_loss: 1.6274]\n",
      "Where on his patence: the prince heavens, and slave wuck it.\n",
      "\n",
      "CLAUDIO:\n",
      "O, gend,\n",
      "But so, who his place, \n",
      "\n",
      "[118m 48s (4450 89%) train loss: 1.4957, test_loss: 1.6043]\n",
      "What sworn and but the kent holy too revenge\n",
      "So wrong, for the Daught thy father pringe.\n",
      "\n",
      "MARGARET:\n",
      "Wh \n",
      "\n",
      "[119m 56s (4500 90%) train loss: 1.5011, test_loss: 1.6316]\n",
      "Where he'll great so son fellow here;\n",
      "The friend it it three effects in the mistress.\n",
      "\n",
      "GLEON:\n",
      "The righ \n",
      "\n",
      "[121m 4s (4550 91%) train loss: 1.4834, test_loss: 1.6140]\n",
      "When I would to report to so' thy prick by me were in we\n",
      "love with our wantation of quart come to war  \n",
      "\n",
      "[122m 12s (4600 92%) train loss: 1.5283, test_loss: 1.6321]\n",
      "What like the please what is not the voice\n",
      "That offers to you.\n",
      "\n",
      "PRINCE HENRY:\n",
      "I kee in the sliting mad \n",
      "\n",
      "[123m 21s (4650 93%) train loss: 1.4833, test_loss: 1.6337]\n",
      "Whath, my botter'd that woudghich,\n",
      "To perfect was, to-dadg too?\n",
      "\n",
      "Do nee! for known this,\n",
      "To sorrow wit \n",
      "\n",
      "[124m 29s (4700 94%) train loss: 1.5206, test_loss: 1.6590]\n",
      "What you am the grace thou have a me;\n",
      "And whose turn him here of justick shall\n",
      "do not whos him; I am b \n",
      "\n",
      "[125m 36s (4750 95%) train loss: 1.4817, test_loss: 1.6066]\n",
      "Whow where you come this to my had man in her of\n",
      "glas Bandray honour have revenge excecked to art;\n",
      "I w \n",
      "\n",
      "[126m 44s (4800 96%) train loss: 1.5606, test_loss: 1.6416]\n",
      "What to unhings their growing will be\n",
      "Tell to gavelicitany has cannot be all me,\n",
      "To most? My lord; I h \n",
      "\n",
      "[127m 52s (4850 97%) train loss: 1.4984, test_loss: 1.6463]\n",
      "What's you of grosle, criets will there shall\n",
      "like a chuse: and I have tell this play you,\n",
      "And who hea \n",
      "\n",
      "[129m 1s (4900 98%) train loss: 1.5217, test_loss: 1.6113]\n",
      "Where well my promish here, and belood.\n",
      "\n",
      "SICNT:\n",
      "Who the king-bot in loss and some bed with my judge!\n",
      "T \n",
      "\n",
      "[130m 9s (4950 99%) train loss: 1.4949, test_loss: 1.5962]\n",
      "Where is a tames in me of the\n",
      "shals words. Thanks with this marks unform.\n",
      "\n",
      "Third:\n",
      "It is a boy, but now \n",
      "\n",
      "[131m 17s (5000 100%) train loss: 1.5107, test_loss: 1.6370]\n",
      "What heat, gentle will storms of his oath\n",
      "proclain by this dog cold with Barm for from my grates\n",
      "Which \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(n_characters, hidden_size, n_characters, model_type=model_type, n_layers=n_layers).to(device)\n",
    "rnn_optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "test_losses = []\n",
    "loss_avg = 0\n",
    "test_loss_avg = 0\n",
    "\n",
    "\n",
    "print(\"Training for %d epochs...\" % n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(rnn, *load_random_batch(train_text, chunk_len, batch_size), rnn_optimizer, criterion)\n",
    "    loss_avg += loss\n",
    "    \n",
    "    test_loss = eval_test(rnn, *load_random_batch(test_text, chunk_len, batch_size))\n",
    "    test_loss_avg += test_loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) train loss: %.4f, test_loss: %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, test_loss))\n",
    "        print(generate(rnn, 'Wh', 100, device=device), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        test_losses.append(test_loss_avg / plot_every)\n",
    "        loss_avg = 0\n",
    "        test_loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-13a9d28916eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# save network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./rnn_generator.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# save network\n",
    "torch.save(rnn.state_dict(), './rnn_generator.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Training and Test Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.plot(test_losses, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate text generation\n",
    "\n",
    "Check what the outputted text looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate(rnn, prime_str='Th', predict_len=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Some things you should try to improve your network performance are:\n",
    "- Different RNN types. Switch the basic RNN network in your model to a GRU and LSTM to compare all three.\n",
    "- Try adding 1 or two more layers\n",
    "- Increase the hidden layer size\n",
    "- Changing the learning rate\n",
    "\n",
    "**TODO:** Try changing the RNN type and hyperparameters. Record your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
