{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import glob\n",
    "import string\n",
    "import random \n",
    "# !conda install torch, torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "from rnn.helpers import time_since\n",
    "# !conda install unidecode\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language recognition with an RNN\n",
    "\n",
    "If you've ever used an online translator you've probably seen a feature that automatically detects the input language. While this might be easy to do if you input unicode characters that are unique to one or a small group of languages (like \"你好\" or \"γεια σας\"), this problem is more challenging if the input only uses the available ASCII characters. In this case, something like \"těší mě\" would beome \"tesi me\" in the ascii form. This is a more challenging problem in which the language must be recognized purely by the pattern of characters rather than unique unicode characters.\n",
    "\n",
    "We will train an RNN to solve this problem for a small set of languages thta can be converted to romanized ASCII form. For training data it would be ideal to have a large and varied dataset in different language styles. However, it is easy to find copies of the Bible which is a large text translated to different languages but in the same easily parsable format, so we will use 20 different copies of the Bible as training data. Using the same book for all of the different languages will hopefully prevent minor overfitting that might arise if we used different books for each language (fitting to common characteristics of the individual books rather than the language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tesi me\n"
     ]
    }
   ],
   "source": [
    "from unidecode import unidecode as unicodeToAscii\n",
    "\n",
    "all_characters = string.printable\n",
    "n_letters = len(all_characters)\n",
    "\n",
    "print(unicodeToAscii('těší mě'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a file and split into lines\n",
    "def readFile(filename):\n",
    "    data = open(filename, encoding='utf-8').read().strip()\n",
    "    return unicodeToAscii(data)\n",
    "\n",
    "def get_category_data(data_path):\n",
    "    # Build the category_data dictionary, a list of names per language\n",
    "    category_data = {}\n",
    "    all_categories = []\n",
    "    for filename in glob.glob(data_path):\n",
    "        category = os.path.splitext(os.path.basename(filename))[0].split('_')[0]\n",
    "        all_categories.append(category)\n",
    "        data = readFile(filename)\n",
    "        category_data[category] = data\n",
    "    \n",
    "    return category_data, all_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original text is split into two parts, train and test, so that we can make sure that the model is not simply memorizing the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "['albanian', 'czech', 'danish', 'english', 'esperanto', 'finnish', 'french', 'german', 'hungarian', 'italian', 'lithuanian', 'maori', 'norwegian', 'portuguese', 'romanian', 'spanish', 'swedish', 'turkish', 'vietnamese', 'xhosa']\n"
     ]
    }
   ],
   "source": [
    "train_data_path = 'language_data/train/*_train.txt'\n",
    "test_data_path = 'language_data/test/*_test.txt'\n",
    "\n",
    "train_category_data, all_categories = get_category_data(train_data_path)\n",
    "test_category_data, test_all_categories = get_category_data(test_data_path)\n",
    "\n",
    "n_languages = len(all_categories)\n",
    "\n",
    "print(len(all_categories))\n",
    "print(all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1, dim=1)\n",
    "    category_i = top_i[:, 0]\n",
    "    return category_i\n",
    "\n",
    "# Turn string into long tensor\n",
    "def stringToTensor(string):\n",
    "    tensor = torch.zeros(len(string), requires_grad=True).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor\n",
    "\n",
    "def load_random_batch(text, chunk_len, batch_size):\n",
    "    input_data = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    target = torch.zeros(batch_size, 1).long().to(device)\n",
    "    input_text = []\n",
    "    for i in range(batch_size):\n",
    "        category = all_categories[random.randint(0, len(all_categories) - 1)]\n",
    "        line_start = random.randint(0, len(text[category])-chunk_len)\n",
    "        category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "        line = text[category][line_start:line_start+chunk_len]\n",
    "        input_text.append(line)\n",
    "        input_data[i] = stringToTensor(line)\n",
    "        target[i] = category_tensor\n",
    "    return input_data, target, input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Model\n",
    "====================\n",
    "\n",
    "For this classification task, we can use the same model we implement for the generation task which is located in `rnn/model.py`. See the `MP4_P2_generation.ipynb` notebook for more instructions. In this case each output vector of our RNN will have the dimension of the number of possible languages (i.e. `n_languages`). We will use this vector to predict a distribution over the languages.\n",
    "\n",
    "In the generation task, we used the output of the RNN at every time step to predict the next letter and our loss included the output from each of these predictions. However, in this task we use the output of the RNN at the end of the sequence to predict the language, so our loss function will use only the predicted output from the last time step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn.model import RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_len = 50\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "n_epochs = 4000\n",
    "hidden_size = 200\n",
    "n_layers = 3\n",
    "learning_rate = 0.0005\n",
    "model_type = 'gru'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "rnn = RNN(n_letters, hidden_size, n_languages, model_type=model_type, n_layers=n_layers).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Fill in the train function. You should initialize a hidden layer representation using your RNN's `init_hidden` function, set the model gradients to zero, and loop over each time step (character) in the input tensor. For each time step compute the output of the of the RNN and the next hidden layer representation. The cross entropy loss should be computed over the last RNN output scores from the end of the sequence and the target classification tensor. Lastly, call backward on the loss and take an optimizer step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, target_tensor, data_tensor, optimizer, criterion, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - rnn: model\n",
    "    - target_target: target character data tensor of shape (batch_size, 1)\n",
    "    - data_tensor: input character data tensor of shape (batch_size, chunk_len)\n",
    "    - optimizer: rnn model optimizer\n",
    "    - criterion: loss function\n",
    "    - batch_size: data batch size\n",
    "    \n",
    "    Returns:\n",
    "    - output: output from RNN from end of sequence \n",
    "    - loss: computed loss value as python float\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    output, loss = None, None\n",
    "    \n",
    "    ####################################\n",
    "    #          YOUR CODE HERE          #\n",
    "    ####################################\n",
    "    hidden = rnn.init_hidden(data_tensor.shape[0])\n",
    "    rnn.zero_grad()\n",
    "    for i in range(chunk_len):\n",
    "        output, hidden = rnn(data_tensor[:, i], hidden)\n",
    "    loss = criterion(output.reshape(batch_size, -1), target_tensor.squeeze())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    ##########       END      ##########\n",
    "\n",
    "    return output, loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(rnn, data_tensor, seq_len=chunk_len, batch_size=BATCH_SIZE):\n",
    "    with torch.no_grad():\n",
    "        data_tensor = data_tensor.to(device)\n",
    "        hidden = rnn.init_hidden(batch_size, device=device)\n",
    "        for i in range(seq_len):\n",
    "            output, hidden = rnn(data_tensor[:,i], hidden)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "def eval_test(rnn, category_tensor, data_tensor):\n",
    "    with torch.no_grad():\n",
    "        output = evaluate(rnn, data_tensor)\n",
    "        loss = criterion(output, category_tensor.squeeze())\n",
    "        return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 1% (3m 13s) 1.9963 1.9288 ezusi e nisi te shkoje ne shtepine e tij duke i th / german ✗ (albanian)\n",
      "Train accuracy: 0.1976\n",
      "100 2% (5m 51s) 1.3320 1.2487 as grandes, e esconde-as com barro no pavimento qu / portuguese ✓\n",
      "Train accuracy: 0.49\n",
      "150 3% (8m 39s) 1.0519 0.9935 Siria kiralya ellen harczolt. Akhazia pedig, Joram / turkish ✗ (hungarian)\n",
      "Train accuracy: 0.6046\n",
      "200 5% (11m 33s) 0.7739 0.7192 stines went to their own place. Now when Saul had  / english ✓\n",
      "Train accuracy: 0.6872\n",
      "250 6% (14m 39s) 0.6508 0.6251 is princerve. Ata ishin te regjistruar sipas gjene / albanian ✓\n",
      "Train accuracy: 0.7478\n",
      "Test accuracy:  0.574\n",
      "300 7% (18m 33s) 0.5826 0.5354 Pak Samson pevne objal oba prostredni sloupy, na n / czech ✓\n",
      "Train accuracy: 0.7922\n",
      "350 8% (21m 51s) 0.4038 0.3591  a ira do Senhor contra o seu povo, de modo que ab / portuguese ✓\n",
      "Train accuracy: 0.8316\n",
      "400 10% (25m 16s) 0.3718 0.3098 byly dcery Jobovy. Otec jim dal dedictvi jako jeji / czech ✓\n",
      "Train accuracy: 0.8594\n",
      "450 11% (28m 55s) 0.3803 0.3130 ael, dupa semintiile lui, si a fost aratata prin s / romanian ✓\n",
      "Train accuracy: 0.8606\n",
      "500 12% (32m 45s) 0.2536 0.2391 atok a halottat es ne bankodjatok erte, hanem azt  / hungarian ✓\n",
      "Train accuracy: 0.882\n",
      "Test accuracy:  0.726\n",
      "550 13% (37m 38s) 0.2061 0.1916 t midianilaisten vanhimmille viestin: \"Nyt tuo jou / finnish ✓\n",
      "Train accuracy: 0.903\n",
      "600 15% (42m 2s) 0.2327 0.2069 se shpirti im gjen strehe te ti; gjej strehe ne hi / albanian ✓\n",
      "Train accuracy: 0.9082\n",
      "650 16% (46m 41s) 0.2691 0.2568  Israel til, sa at de kraenkede HERREN, Israels Gu / danish ✓\n",
      "Train accuracy: 0.9086\n",
      "700 17% (51m 33s) 0.2710 0.2650 m var son av Matusala, som var son av Enok, som va / swedish ✓\n",
      "Train accuracy: 0.9222\n",
      "750 18% (56m 20s) 0.1838 0.1654  kha can dam; con nguoi, Gie-ho-sua, con trai Gio- / vietnamese ✓\n",
      "Train accuracy: 0.929\n",
      "Test accuracy:  0.746\n",
      "800 20% (62m 18s) 0.1395 0.1233 u me vrap duke i thene Ashaziahut: \"Tradheti, Asha / albanian ✓\n",
      "Train accuracy: 0.9334\n",
      "850 21% (67m 39s) 0.1928 0.1539  gasell- eller hjortkott; bade den som ar oren och / swedish ✓\n",
      "Train accuracy: 0.9368\n",
      "900 22% (73m 11s) 0.2048 0.1596 iolo al mozo, y diose este priesa a aderezarlo. To / spanish ✓\n",
      "Train accuracy: 0.9412\n",
      "950 23% (78m 46s) 0.1057 0.1035 cinq mille de longueur, seront un espace non consa / french ✓\n",
      "Train accuracy: 0.944\n",
      "1000 25% (84m 26s) 0.1613 0.1257 o, kie oni supreniras al la armilejo. Post li vigl / esperanto ✓\n",
      "Train accuracy: 0.9468\n",
      "Test accuracy:  0.784\n",
      "1050 26% (91m 5s) 0.1128 0.0981 c su do? Thien su truyen rang: Duc Thanh Linh se d / vietnamese ✓\n",
      "Train accuracy: 0.9466\n",
      "1100 27% (97m 2s) 0.0931 0.0772  Mas eles disseram: Nao andaremos nele. Tambem pus / portuguese ✓\n",
      "Train accuracy: 0.9524\n",
      "1150 28% (103m 14s) 0.1174 0.1056  kom og fridde ut, sa det skulde vaere hans eget f / norwegian ✓\n",
      "Train accuracy: 0.9526\n",
      "1200 30% (109m 31s) 0.1769 0.1519 e avete abbandonato l'Eterno, anch'egli vi abbando / italian ✓\n",
      "Train accuracy: 0.9526\n",
      "1250 31% (115m 40s) 0.1611 0.1149 j de la Izraelidoj, kaj cxi tiuj ne mortigis ilin. / esperanto ✓\n",
      "Train accuracy: 0.9592\n",
      "Test accuracy:  0.8\n",
      "1300 32% (123m 2s) 0.0818 0.0733 e tetahi i roto i tenei whare e nui ake i ahau; ka / maori ✓\n",
      "Train accuracy: 0.9616\n",
      "1350 33% (129m 44s) 0.1205 0.1069  och bliva helade av mig. Men saliga aro edra ogon / swedish ✓\n",
      "Train accuracy: 0.9622\n",
      "1400 35% (136m 34s) 0.0720 0.0623 ille gora detsamma genom sina hemliga konster och  / swedish ✓\n",
      "Train accuracy: 0.957\n",
      "1450 36% (143m 32s) 0.0857 0.0798 pper og sammenkomstens telt med dekket og varetake / norwegian ✓\n",
      "Train accuracy: 0.9652\n",
      "1500 37% (150m 42s) 0.1131 0.0974 ao bi nguoi nu sanh ra, sao cho la trong sach duoc / vietnamese ✓\n",
      "Train accuracy: 0.9648\n",
      "Test accuracy:  0.813\n",
      "1550 38% (158m 42s) 0.0946 0.0646 t``. Poporul a venit la Betel, si a stat inaintea  / romanian ✓\n",
      "Train accuracy: 0.9622\n",
      "1600 40% (166m 4s) 0.1841 0.1704 Doan, chung khoi hanh. Duc Chua Troi ben giang su  / vietnamese ✓\n",
      "Train accuracy: 0.9668\n",
      "1650 41% (173m 33s) 0.1698 0.1430  benjaminitten Kusj' ord.) HERRE min Gud, jeg lide / esperanto ✗ (danish)\n",
      "Train accuracy: 0.9672\n",
      "1700 42% (180m 39s) 0.0434 0.0371 wenza umfanekiso oqingqiweyo notyhidiweyo, into el / xhosa ✓\n",
      "Train accuracy: 0.9678\n",
      "1750 43% (186m 58s) 0.0597 0.0448 ali os devorarei como leoa; as feras do campo os d / portuguese ✓\n",
      "Train accuracy: 0.9694\n",
      "Test accuracy:  0.825\n",
      "1800 45% (193m 20s) 0.0700 0.0442 zeg, es meglodul, mint a kaliba, es rea nehezedik  / hungarian ✓\n",
      "Train accuracy: 0.971\n",
      "1850 46% (199m 18s) 0.0404 0.0424  Dievas kelesi teisti ir isgelbeti zemes romiuju.  / lithuanian ✓\n",
      "Train accuracy: 0.9692\n",
      "1900 47% (205m 25s) 0.0886 0.0547  la Eternulo Cebaot, Dio de Izrael:Pro tio, ke vi  / esperanto ✓\n",
      "Train accuracy: 0.9708\n",
      "1950 48% (211m 38s) 0.0167 0.0156 uando la desechare. Mas si la hubiere desposado co / spanish ✓\n",
      "Train accuracy: 0.9716\n",
      "2000 50% (217m 57s) 0.1719 0.1183 en beide in die Grube fallen. Da sprach Petrus zu  / german ✓\n",
      "Train accuracy: 0.9768\n",
      "Test accuracy:  0.846\n",
      "2050 51% (224m 49s) 0.0690 0.0587 stes sao os filhos da provincia que subiram do cat / portuguese ✓\n",
      "Train accuracy: 0.9784\n",
      "2100 52% (231m 24s) 0.0419 0.0340 ra um dos que estavam  mesa com ele. Entao Maria,  / portuguese ✓\n",
      "Train accuracy: 0.976\n",
      "2150 53% (238m 7s) 0.0517 0.0444 as, og Jonas var i fiskens buk tre dager og tre ne / norwegian ✓\n",
      "Train accuracy: 0.9742\n",
      "2200 55% (244m 58s) 0.1240 0.0871 g Drikoffer. Pa den anden Dag skal I ofre tolv ung / danish ✓\n",
      "Train accuracy: 0.977\n",
      "2250 56% (251m 56s) 0.0685 0.0464 werde? Denn wer ist mir gleich, und wer will mich  / german ✓\n",
      "Train accuracy: 0.9758\n",
      "Test accuracy:  0.833\n",
      "2300 57% (259m 22s) 0.0186 0.0158 auxlo volis eniri antaux la popolon, la discxiploj / esperanto ✓\n",
      "Train accuracy: 0.9758\n",
      "2350 58% (266m 32s) 0.0572 0.0403 nd mich ihrer erbarmen! Dies ist das Wort, welches / german ✓\n",
      "Train accuracy: 0.9762\n",
      "2400 60% (273m 48s) 0.0756 0.0496 vsech predaku dvanact holi za jejich otcovske rody / czech ✓\n",
      "Train accuracy: 0.9768\n",
      "2450 61% (281m 11s) 0.0238 0.0141 tim no ma chang thay; nhung ke giao chien cung ngu / vietnamese ✓\n",
      "Train accuracy: 0.9818\n",
      "2500 62% (288m 41s) 0.0728 0.0528 o tak, ze by zemrel, ten at se utece do jednoho z  / czech ✓\n",
      "Train accuracy: 0.9794\n",
      "Test accuracy:  0.814\n",
      "Test accuracy:  0.86\n",
      "Test accuracy:  0.839\n",
      "Test accuracy:  0.842\n",
      "Test accuracy:  0.854\n",
      "Test accuracy:  0.857\n",
      "Test accuracy:  0.854\n",
      "Test accuracy:  0.839\n",
      "Test accuracy:  0.849\n",
      "Test accuracy:  0.852\n",
      "Test accuracy:  0.856\n",
      "Test accuracy:  0.847\n",
      "Test accuracy:  0.847\n",
      "Test accuracy:  0.854\n",
      "Test accuracy:  0.855\n",
      "Test accuracy:  0.856\n",
      "Test accuracy:  0.877\n",
      "Test accuracy:  0.864\n",
      "Test accuracy:  0.869\n",
      "Test accuracy:  0.852\n",
      "Test accuracy:  0.851\n",
      "Test accuracy:  0.835\n",
      "Test accuracy:  0.832\n",
      "Test accuracy:  0.88\n",
      "Test accuracy:  0.831\n",
      "Test accuracy:  0.831\n",
      "Test accuracy:  0.848\n",
      "Test accuracy:  0.846\n",
      "Test accuracy:  0.863\n",
      "Test accuracy:  0.856\n",
      "Test accuracy:  0.854\n",
      "Test accuracy:  0.848\n",
      "Test accuracy:  0.855\n",
      "Test accuracy:  0.845\n",
      "Test accuracy:  0.849\n",
      "Test accuracy:  0.842\n",
      "Test accuracy:  0.841\n",
      "Test accuracy:  0.848\n",
      "Test accuracy:  0.85\n",
      "Test accuracy:  0.837\n",
      "Test accuracy:  0.836\n",
      "Test accuracy:  0.843\n",
      "Test accuracy:  0.845\n",
      "Test accuracy:  0.855\n",
      "Test accuracy:  0.853\n",
      "Test accuracy:  0.847\n",
      "Test accuracy:  0.85\n",
      "Test accuracy:  0.838\n",
      "Test accuracy:  0.837\n",
      "Test accuracy:  0.857\n",
      "2550 63% (315m 9s) 0.1124 0.0756 ddet, men Folket gik fri. Saul sagde da: \"Kast Lod / danish ✓\n",
      "Train accuracy: 0.979\n",
      "Test accuracy:  0.849\n",
      "Test accuracy:  0.837\n",
      "Test accuracy:  0.851\n",
      "Test accuracy:  0.843\n",
      "Test accuracy:  0.825\n",
      "Test accuracy:  0.869\n",
      "Test accuracy:  0.845\n",
      "Test accuracy:  0.851\n",
      "Test accuracy:  0.847\n",
      "Test accuracy:  0.833\n",
      "Test accuracy:  0.853\n",
      "Test accuracy:  0.848\n",
      "Test accuracy:  0.853\n",
      "Test accuracy:  0.852\n",
      "Test accuracy:  0.843\n",
      "Test accuracy:  0.852\n",
      "Test accuracy:  0.84\n",
      "Test accuracy:  0.865\n",
      "Test accuracy:  0.831\n",
      "Test accuracy:  0.827\n",
      "Test accuracy:  0.858\n",
      "Test accuracy:  0.847\n",
      "Test accuracy:  0.851\n",
      "Test accuracy:  0.824\n",
      "Test accuracy:  0.829\n",
      "Test accuracy:  0.858\n",
      "Test accuracy:  0.858\n",
      "Test accuracy:  0.856\n",
      "Test accuracy:  0.838\n",
      "Test accuracy:  0.848\n",
      "Test accuracy:  0.829\n",
      "Test accuracy:  0.863\n",
      "Test accuracy:  0.859\n",
      "Test accuracy:  0.824\n",
      "Test accuracy:  0.845\n",
      "Test accuracy:  0.844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.855\n",
      "Test accuracy:  0.848\n",
      "Test accuracy:  0.846\n",
      "Test accuracy:  0.856\n",
      "Test accuracy:  0.869\n",
      "Test accuracy:  0.854\n",
      "Test accuracy:  0.866\n",
      "Test accuracy:  0.859\n",
      "Test accuracy:  0.859\n",
      "Test accuracy:  0.846\n",
      "Test accuracy:  0.864\n",
      "Test accuracy:  0.858\n",
      "Test accuracy:  0.865\n",
      "Test accuracy:  0.876\n",
      "2600 65% (344m 42s) 0.0717 0.0499 m og praedikede Riget. Derfor vidner jeg for eder  / danish ✓\n",
      "Train accuracy: 0.9778\n",
      "Test accuracy:  0.848\n",
      "Test accuracy:  0.867\n",
      "Test accuracy:  0.855\n",
      "Test accuracy:  0.835\n",
      "Test accuracy:  0.848\n",
      "Test accuracy:  0.854\n",
      "Test accuracy:  0.859\n",
      "Test accuracy:  0.864\n",
      "Test accuracy:  0.836\n",
      "Test accuracy:  0.853\n",
      "Test accuracy:  0.857\n",
      "Test accuracy:  0.852\n",
      "Test accuracy:  0.863\n",
      "Test accuracy:  0.844\n",
      "Test accuracy:  0.834\n",
      "Test accuracy:  0.835\n",
      "Test accuracy:  0.845\n",
      "Test accuracy:  0.833\n",
      "Test accuracy:  0.841\n",
      "Test accuracy:  0.848\n",
      "Test accuracy:  0.836\n",
      "Test accuracy:  0.831\n",
      "Test accuracy:  0.827\n",
      "Test accuracy:  0.831\n",
      "Test accuracy:  0.854\n",
      "Test accuracy:  0.868\n",
      "Test accuracy:  0.871\n",
      "Test accuracy:  0.841\n",
      "Test accuracy:  0.84\n",
      "Test accuracy:  0.861\n",
      "Test accuracy:  0.826\n",
      "Test accuracy:  0.843\n",
      "Test accuracy:  0.849\n",
      "Test accuracy:  0.838\n",
      "Test accuracy:  0.851\n",
      "Test accuracy:  0.852\n",
      "Test accuracy:  0.848\n",
      "Test accuracy:  0.844\n",
      "Test accuracy:  0.829\n",
      "Test accuracy:  0.82\n",
      "Test accuracy:  0.837\n",
      "Test accuracy:  0.833\n",
      "Test accuracy:  0.847\n",
      "Test accuracy:  0.845\n",
      "Test accuracy:  0.825\n",
      "Test accuracy:  0.833\n",
      "Test accuracy:  0.855\n",
      "Test accuracy:  0.848\n",
      "Test accuracy:  0.833\n",
      "Test accuracy:  0.877\n",
      "2650 66% (379m 9s) 0.0805 0.0552 l I ha til fode; likesom jeg gav eder de gronne ur / norwegian ✓\n",
      "Train accuracy: 0.9782\n",
      "Test accuracy:  0.856\n",
      "Test accuracy:  0.855\n",
      "Test accuracy:  0.854\n",
      "Test accuracy:  0.868\n",
      "Test accuracy:  0.856\n",
      "Test accuracy:  0.849\n",
      "Test accuracy:  0.835\n",
      "Test accuracy:  0.846\n",
      "Test accuracy:  0.846\n",
      "Test accuracy:  0.838\n",
      "Test accuracy:  0.849\n",
      "Test accuracy:  0.838\n",
      "Test accuracy:  0.854\n",
      "Test accuracy:  0.867\n",
      "Test accuracy:  0.852\n",
      "Test accuracy:  0.847\n",
      "Test accuracy:  0.851\n",
      "Test accuracy:  0.844\n",
      "Test accuracy:  0.862\n",
      "Test accuracy:  0.832\n",
      "Test accuracy:  0.844\n",
      "Test accuracy:  0.855\n",
      "Test accuracy:  0.856\n",
      "Test accuracy:  0.848\n",
      "Test accuracy:  0.852\n",
      "Test accuracy:  0.844\n",
      "Test accuracy:  0.848\n",
      "Test accuracy:  0.857\n",
      "Test accuracy:  0.851\n",
      "Test accuracy:  0.861\n",
      "Test accuracy:  0.867\n",
      "Test accuracy:  0.879\n",
      "Test accuracy:  0.841\n",
      "Test accuracy:  0.86\n",
      "Test accuracy:  0.844\n",
      "Test accuracy:  0.843\n",
      "Test accuracy:  0.852\n",
      "Test accuracy:  0.844\n",
      "Test accuracy:  0.842\n",
      "Test accuracy:  0.863\n",
      "Test accuracy:  0.847\n",
      "Test accuracy:  0.861\n",
      "Test accuracy:  0.857\n",
      "Test accuracy:  0.868\n",
      "Test accuracy:  0.864\n",
      "Test accuracy:  0.845\n",
      "Test accuracy:  0.85\n",
      "Test accuracy:  0.849\n",
      "Test accuracy:  0.841\n",
      "Test accuracy:  0.862\n",
      "2700 67% (413m 13s) 0.0221 0.0207 water; and thou shalt stand by the river's brink t / english ✓\n",
      "Train accuracy: 0.9806\n",
      "Test accuracy:  0.818\n",
      "Test accuracy:  0.877\n",
      "Test accuracy:  0.851\n",
      "Test accuracy:  0.834\n",
      "Test accuracy:  0.844\n",
      "Test accuracy:  0.858\n",
      "Test accuracy:  0.857\n",
      "Test accuracy:  0.857\n",
      "Test accuracy:  0.86\n",
      "Test accuracy:  0.855\n",
      "Test accuracy:  0.894\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'r' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-37f7ebe7fc1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test accuracy: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_correct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_confusion\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0meval_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_correct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_confusion\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0meval_batch_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m.89\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mkaggle_test_file_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'language_data/kaggle_rnn_language_classification_test.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'r' is not defined"
     ]
    }
   ],
   "source": [
    "n_iters = 4000 #100000\n",
    "print_every = 50\n",
    "plot_every = 50\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "current_test_loss = 0\n",
    "all_losses = []\n",
    "all_test_losses = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "number_correct = 0\n",
    "for iter in range(1, n_iters + 1):\n",
    "    input_data, target_category, text_data = load_random_batch(train_category_data, chunk_len, BATCH_SIZE)\n",
    "    output, loss = train(rnn, target_category, input_data, optimizer, criterion)\n",
    "    current_loss += loss\n",
    "    \n",
    "    _, test_loss = eval_test(rnn, target_category, input_data)\n",
    "    current_test_loss += test_loss\n",
    "    \n",
    "    guess_i = categoryFromOutput(output)\n",
    "    number_correct += (target_category.squeeze()==guess_i.squeeze()).long().sum()\n",
    "    \n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        sample_idx = 0\n",
    "        guess = all_categories[guess_i[sample_idx]]\n",
    "        \n",
    "        category = all_categories[int(target_category[sample_idx])]\n",
    "        \n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %.4f %s / %s %s' % (iter, iter / n_iters * 100, time_since(start), loss, test_loss, text_data[sample_idx], guess, correct))\n",
    "        print('Train accuracy: {}'.format(float(number_correct)/float(print_every*BATCH_SIZE)))\n",
    "        number_correct = 0\n",
    "    \n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0\n",
    "        all_test_losses.append(current_test_loss / plot_every)\n",
    "        current_test_loss = 0\n",
    "    if iter % (print_every*5) == 0 or (iter > 2500 and iter % (print_every*5)):\n",
    "        eval_batch_size = 1  # needs to be set to 1 for evaluating different sequence lengths\n",
    "\n",
    "        # Keep track of correct guesses in a confusion matrix\n",
    "        confusion = torch.zeros(n_languages, n_languages)\n",
    "        n_confusion = 1000\n",
    "        num_correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for i in range(n_confusion):\n",
    "            eval_chunk_len = random.randint(10, 50) # in evaluation we will look at sequences of variable sizes\n",
    "            input_data, target_category, text_data = load_random_batch(test_category_data, chunk_len=eval_chunk_len, batch_size=eval_batch_size)\n",
    "            output = evaluate(rnn, input_data, seq_len=eval_chunk_len, batch_size=eval_batch_size)\n",
    "\n",
    "            guess_i = categoryFromOutput(output)\n",
    "            category_i = [int(target_category[idx]) for idx in range(len(target_category))]\n",
    "            for j in range(eval_batch_size):\n",
    "                category = all_categories[category_i[j]] \n",
    "                confusion[category_i[j]][guess_i[j]] += 1\n",
    "                num_correct += int(guess_i[j]==category_i[j])\n",
    "                total += 1\n",
    "\n",
    "        print('Test accuracy: ', float(num_correct)/float(n_confusion*eval_batch_size))\n",
    "        if float(num_correct)/float(n_confusion*eval_batch_size) > .89:\n",
    "            r\n",
    "            \n",
    "            kaggle_test_file_path = 'language_data/kaggle_rnn_language_classification_test.txt'\n",
    "            with open(kaggle_test_file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            output_rows = []\n",
    "            for i, line in enumerate(lines):\n",
    "                sample = line.rstrip()\n",
    "                sample_chunk_len = len(sample)\n",
    "                input_data = stringToTensor(sample).unsqueeze(0)\n",
    "                output = evaluate(rnn, input_data, seq_len=sample_chunk_len, batch_size=1)\n",
    "                guess_i = categoryFromOutput(output)\n",
    "                output_rows.append((str(i+1), all_categories[guess_i]))\n",
    "\n",
    "            submission_file_path = 'kaggle_rnn_submission'+str(iter)+'.txt'\n",
    "            with open(submission_file_path, 'w') as f:\n",
    "                output_rows = [('id', 'category')] + output_rows\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerows(output_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss functions\n",
    "--------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses, color='b')\n",
    "plt.plot(all_test_losses, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate results\n",
    "-------------------\n",
    "\n",
    "We now vizualize the performance of our model by creating a confusion matrix. The ground truth languages of samples are represented by rows in the matrix while the predicted languages are represented by columns.\n",
    "\n",
    "In this evaluation we consider sequences of variable sizes rather than the fixed length sequences we used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch_size = 1  # needs to be set to 1 for evaluating different sequence lengths\n",
    "\n",
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_languages, n_languages)\n",
    "n_confusion = 1000\n",
    "num_correct = 0\n",
    "total = 0\n",
    "\n",
    "for i in range(n_confusion):\n",
    "    eval_chunk_len = random.randint(10, 50) # in evaluation we will look at sequences of variable sizes\n",
    "    input_data, target_category, text_data = load_random_batch(test_category_data, chunk_len=eval_chunk_len, batch_size=eval_batch_size)\n",
    "    output = evaluate(rnn, input_data, seq_len=eval_chunk_len, batch_size=eval_batch_size)\n",
    "    \n",
    "    guess_i = categoryFromOutput(output)\n",
    "    category_i = [int(target_category[idx]) for idx in range(len(target_category))]\n",
    "    for j in range(eval_batch_size):\n",
    "        category = all_categories[category_i[j]] \n",
    "        confusion[category_i[j]][guess_i[j]] += 1\n",
    "        num_correct += int(guess_i[j]==category_i[j])\n",
    "        total += 1\n",
    "\n",
    "print('Test accuracy: ', float(num_correct)/float(n_confusion*eval_batch_size))\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_languages):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pick out bright spots off the main axis that show which\n",
    "languages it guesses incorrectly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on User Input\n",
    "---------------------\n",
    "\n",
    "Now you can test your model on your own input. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_line, n_predictions=5):\n",
    "    print('\\n> %s' % input_line)\n",
    "    with torch.no_grad():\n",
    "        input_data = stringToTensor(input_line).long().unsqueeze(0).to(device)\n",
    "        output = evaluate(rnn, input_data, seq_len=len(input_line), batch_size=1)\n",
    "\n",
    "    # Get top N categories\n",
    "    topv, topi = output.topk(n_predictions, dim=1)\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(n_predictions):\n",
    "        topv.shape\n",
    "        topi.shape\n",
    "        value = topv[0][i].item()\n",
    "        category_index = topi[0][i].item()\n",
    "        print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "        predictions.append([value, all_categories[category_index]])\n",
    "\n",
    "predict('This is a phrase to test the model on user input')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Kaggle submission file\n",
    "\n",
    "Once you have found a good set of hyperparameters submit the output of your model on the Kaggle test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT CHANGE KAGGLE SUBMISSION CODE ####\n",
    "import csv\n",
    "\n",
    "kaggle_test_file_path = 'language_data/kaggle_rnn_language_classification_test.txt'\n",
    "with open(kaggle_test_file_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "output_rows = []\n",
    "for i, line in enumerate(lines):\n",
    "    sample = line.rstrip()\n",
    "    sample_chunk_len = len(sample)\n",
    "    input_data = stringToTensor(sample).unsqueeze(0)\n",
    "    output = evaluate(rnn, input_data, seq_len=sample_chunk_len, batch_size=1)\n",
    "    guess_i = categoryFromOutput(output)\n",
    "    output_rows.append((str(i+1), all_categories[guess_i]))\n",
    "\n",
    "submission_file_path = 'kaggle_rnn_submission.txt'\n",
    "with open(submission_file_path, 'w') as f:\n",
    "    output_rows = [('id', 'category')] + output_rows\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(output_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
