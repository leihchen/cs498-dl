{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text with an RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn.model import RNN\n",
    "from rnn.helpers import time_since\n",
    "from rnn.generate import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "The file we are using is a plain text file. We turn any potential unicode characters into plain ASCII by using the `unidecode` package (which you can install via `pip` or `conda`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 4573338\n",
      "train len:  4116004\n",
      "test len:  457334\n"
     ]
    }
   ],
   "source": [
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file_path = './shakespeare.txt'\n",
    "file = unidecode.unidecode(open(file_path).read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)\n",
    "\n",
    "# we will leave the last 1/10th of text as test\n",
    "split = int(0.9*file_len)\n",
    "train_text = file[:split]\n",
    "test_text = file[split:]\n",
    "\n",
    "print('train len: ', len(train_text))\n",
    "print('test len: ', len(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hunter serve your turn?\n",
      "\n",
      "MISTRESS PAGE:\n",
      "I pray you, come, hold up the jest no higher\n",
      "Now, good Sir John, how like you Windsor wives?\n",
      "See you these, husband? do not these fair yokes\n",
      "Become the forest b\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "def random_chunk(text):\n",
    "    start_index = random.randint(0, len(text) - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return text[start_index:end_index]\n",
    "\n",
    "print(random_chunk(train_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make training samples out of the large string of text data, we will be splitting the text into chunks.\n",
    "\n",
    "Each chunk will be turned into a tensor, specifically a `LongTensor` (used for integer values), by looping through the characters of the string and looking up the index of each character in `all_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string), requires_grad=True).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function loads a batch of input and target tensors for training. Each sample comes from a random chunk of text. A sample input will consist of all characters *except the last*, while the target wil contain all characters *following the first*. For example: if random_chunk='abc', then input='ab' and target='bc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_random_batch(text, chunk_len, batch_size):\n",
    "    input_data = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    target = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    for i in range(batch_size):\n",
    "        start_index = random.randint(0, len(text) - chunk_len - 1)\n",
    "        end_index = start_index + chunk_len + 1\n",
    "        chunk = text[start_index:end_index]\n",
    "        input_data[i] = char_tensor(chunk[:-1])\n",
    "        target[i] = char_tensor(chunk[1:])\n",
    "    return input_data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement model\n",
    "\n",
    "Your RNN model will take as input the character for step $t_{-1}$ and output a prediction for the next character $t$. The model should consiste of three layers - a linear layer that encodes the input character into an embedded state, an RNN layer (which may itself have multiple layers) that operates on that embedded state and a hidden state, and a decoder layer that outputs the predicted character scores distribution.\n",
    "\n",
    "\n",
    "You must implement your model in the `rnn/model.py` file. You should use a `nn.Embedding` object for the encoding layer, a RNN model like `nn.RNN` or `nn.LSTM`, and a `nn.Linear` layer for the final a predicted character score decoding layer.\n",
    "\n",
    "\n",
    "**TODO:** Implement the model in RNN `rnn/model.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating\n",
    "\n",
    "To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time.\n",
    "\n",
    "\n",
    "Note that in the `evaluate` function, every time a prediction is made the outputs are divided by the \"temperature\" argument. Higher temperature values make actions more equally likely giving more \"random\" outputs. Lower temperature values (less than 1) high likelihood options contribute more. A temperature near 0 outputs only the most likely outputs.\n",
    "\n",
    "You may check different temperature values yourself, but we have provided a default which should work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(rnn, prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = rnn.init_hidden(1, device=device)\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = rnn(prime_input[p].unsqueeze(0).to(device), hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = rnn(inp.unsqueeze(0).to(device), hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_epochs = 5000\n",
    "hidden_size = 100\n",
    "n_layers = 1\n",
    "learning_rate = 0.01\n",
    "model_type = 'rnn'\n",
    "print_every = 50\n",
    "plot_every = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test(rnn, inp, target):\n",
    "    with torch.no_grad():\n",
    "        hidden = rnn.init_hidden(batch_size, device=device)\n",
    "        loss = 0\n",
    "        for c in range(chunk_len):\n",
    "            output, hidden = rnn(inp[:,c], hidden)\n",
    "            loss += criterion(output.view(batch_size, -1), target[:,c])\n",
    "    \n",
    "    return loss.data.item() / chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function\n",
    "\n",
    "**TODO**: Fill in the train function. You should initialize a hidden layer representation using your RNN's `init_hidden` function, set the model gradients to zero, and loop over each time step (character) in the input tensor. For each time step compute the output of the of the RNN and compute the loss over the output and the corresponding ground truth time step in `target`. The loss should be averaged over all time steps. Lastly, call backward on the averaged loss and take an optimizer step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, input, target, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - rnn: model\n",
    "    - input: input character data tensor of shape (batch_size, chunk_len)\n",
    "    - target: target character data tensor of shape (batch_size, chunk_len)\n",
    "    - optimizer: rnn model optimizer\n",
    "    - criterion: loss function\n",
    "    \n",
    "    Returns:\n",
    "    - loss: computed loss value as python float\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    \n",
    "    ####################################\n",
    "    #          YOUR CODE HERE          #\n",
    "    ####################################\n",
    "    hidden = rnn.init_hidden(input.shape[0])\n",
    "    rnn.zero_grad()\n",
    "    for i in range(chunk_len):\n",
    "        output, hidden = rnn(input[:, i], hidden)\n",
    "        loss += criterion(output.reshape(batch_size, -1), target[:, i]) / chunk_len\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    ##########       END      ##########\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 epochs...\n",
      "[1m 21s (50 1%) train loss: 2.1291, test_loss: 2.1087]\n",
      "Whem readist,\n",
      "Ther compere thou forer wiy; Gom I dow ou pet;\n",
      "And mork shan thours,\n",
      "And, thet cains he  \n",
      "\n",
      "[2m 56s (100 2%) train loss: 1.9401, test_loss: 1.9726]\n",
      "Wh:\n",
      "And come?\n",
      "\n",
      "Fit the more mell;\n",
      "Way of I bestion thou?\n",
      "\n",
      "STICHONIA:\n",
      "The gray you with stame of whey h \n",
      "\n",
      "[4m 42s (150 3%) train loss: 1.8613, test_loss: 1.8877]\n",
      "Whriline whis lece and it canishals, be this refors this of rairs in and tues not is will in from fear \n",
      "\n",
      "[6m 23s (200 4%) train loss: 1.8191, test_loss: 1.8587]\n",
      "What my wast not hand that it all a chame\n",
      ", which fit, he with strive as bear but you\n",
      "worch\n",
      "parnels is \n",
      "\n",
      "[7m 47s (250 5%) train loss: 1.7799, test_loss: 1.8267]\n",
      "Wha kissered compt can not the pach: say and I beith to pere sent the sage his theith the has shall th \n",
      "\n",
      "[9m 9s (300 6%) train loss: 1.7670, test_loss: 1.8234]\n",
      "Whr, stropans, that and\n",
      "Thou dark,\n",
      "Thou art he how he would have the master him and never entred for h \n",
      "\n",
      "[10m 55s (350 7%) train loss: 1.7685, test_loss: 1.7952]\n",
      "Whid well shall newn:\n",
      "Mited them.\n",
      "\n",
      "THLES:\n",
      "It inced.\n",
      "\n",
      "GLOUCESTER:\n",
      "What he you not best to him and this  \n",
      "\n",
      "[13m 8s (400 8%) train loss: 1.6994, test_loss: 1.7875]\n",
      "Why:\n",
      "I am been have morner sweet you do\n",
      "Mut the bear Prochear entred, I proke any to to lown: and her, \n",
      "\n",
      "[14m 52s (450 9%) train loss: 1.7199, test_loss: 1.7594]\n",
      "Whill were must did so more bear free ender thee,\n",
      "When will dother by friendstated with the conveers,  \n",
      "\n",
      "[16m 15s (500 10%) train loss: 1.6960, test_loss: 1.7664]\n",
      "Who to of this like be might ent,\n",
      "Or nobla ones to Mastice my worney though a trunish, I weathing-make \n",
      "\n",
      "[17m 25s (550 11%) train loss: 1.7079, test_loss: 1.7598]\n",
      "Whip he have be you here meat.\n",
      "\n",
      "Sy Dellus to weep a sairtes his grace, be thy good fattand!\n",
      "Noth some  \n",
      "\n",
      "[18m 33s (600 12%) train loss: 1.6912, test_loss: 1.7442]\n",
      "Where,\n",
      "O the here\n",
      "Part this still the dose of the follow master and in this barnest me bribe of the fa \n",
      "\n",
      "[19m 38s (650 13%) train loss: 1.6908, test_loss: 1.7722]\n",
      "What is the go more you whom sicks once mine court to the groaboy dause not he will gedper:\n",
      "Detly you  \n",
      "\n",
      "[20m 43s (700 14%) train loss: 1.6870, test_loss: 1.7698]\n",
      "Who subd, on unand, ream, you,\n",
      "The seek to the wert thou sonery our rest;\n",
      "Kin:\n",
      "What much her us a wort \n",
      "\n",
      "[21m 50s (750 15%) train loss: 1.6847, test_loss: 1.7773]\n",
      "WhaZIA:\n",
      "Some:\n",
      "I she me; an I have that condeent?\n",
      "\n",
      "BOY:\n",
      "Brown, them good be hear alsed it, but have but \n",
      "\n",
      "[22m 54s (800 16%) train loss: 1.6588, test_loss: 1.7706]\n",
      "What be strear upon dawlers.\n",
      "\n",
      "CRESSID:\n",
      "And lark for a boot,\n",
      "If Jown to this\n",
      "To-hrown and propers, ther \n",
      "\n",
      "[24m 1s (850 17%) train loss: 1.6654, test_loss: 1.7824]\n",
      "Why here with you,\n",
      "And or your with he shall best me the sigices, thy give with this not after side, s \n",
      "\n",
      "[25m 18s (900 18%) train loss: 1.6623, test_loss: 1.7497]\n",
      "What eadn is tear thiend and the greatious scook, tas his succomwarmok with my love no be I have feor  \n",
      "\n",
      "[26m 32s (950 19%) train loss: 1.6617, test_loss: 1.7449]\n",
      "What that resticters of righate him the king the propery heng.\n",
      "Good by her grace it as what she was is \n",
      "\n",
      "[27m 45s (1000 20%) train loss: 1.6589, test_loss: 1.7526]\n",
      "What confaldon a discroter as in Dartfill of thy part at here a call thou rojext the bittle gone's lee \n",
      "\n",
      "[28m 58s (1050 21%) train loss: 1.6596, test_loss: 1.7467]\n",
      "What the love.\n",
      "Now I like: thou know's atired that bloody whose all: but issar I have will pitient ecr \n",
      "\n",
      "[30m 12s (1100 22%) train loss: 1.6658, test_loss: 1.7308]\n",
      "Wha by prithou servant; the chole most but a fellow, my more bestalls fietfsan field my cear well you  \n",
      "\n",
      "[31m 24s (1150 23%) train loss: 1.6350, test_loss: 1.7166]\n",
      "WhBURINA:\n",
      "With his mare are heart\n",
      "De an my lord,\n",
      "but the keeby take on combout comes behold in be not  \n",
      "\n",
      "[32m 37s (1200 24%) train loss: 1.6285, test_loss: 1.7261]\n",
      "What He would be mon ill calse reath, I sware his more thee no servant.\n",
      "\n",
      "DUFILUS:\n",
      "Nare with the lord.\n",
      " \n",
      "\n",
      "[33m 50s (1250 25%) train loss: 1.6497, test_loss: 1.7480]\n",
      "What in villanch villow in his true sholl, lord, by Johe any apsing by my him, that shall fight it wil \n",
      "\n",
      "[35m 5s (1300 26%) train loss: 1.6702, test_loss: 1.7523]\n",
      "Where so hearte inferclain.\n",
      "\n",
      "Third as the king upon some my more of villainst a stay might thou spir m \n",
      "\n",
      "[36m 18s (1350 27%) train loss: 1.6632, test_loss: 1.7606]\n",
      "Wherse senations and, untolant of the not she hast should and in this news aid she as comes the hear o \n",
      "\n",
      "[37m 31s (1400 28%) train loss: 1.6416, test_loss: 1.7137]\n",
      "When do ender'd my son;\n",
      "Which I be our so mark to him must I drien.\n",
      "\n",
      "FLUELLEN:\n",
      "Be thee,\n",
      "you are the ve \n",
      "\n",
      "[38m 45s (1450 28%) train loss: 1.6479, test_loss: 1.7413]\n",
      "Why devery;\n",
      "Till you world reath the care in tong my good to the senteed it in our gracts so lady.\n",
      "\n",
      "CL \n",
      "\n",
      "[39m 59s (1500 30%) train loss: 1.6365, test_loss: 1.7062]\n",
      "When their an the villance the lordson:\n",
      "You may heart into asires,\n",
      "Which the blord.\n",
      "\n",
      "THERSITED:\n",
      "Do in  \n",
      "\n",
      "[40m 56s (1550 31%) train loss: 1.6477, test_loss: 1.7200]\n",
      "Whal hoped my fall done and for a honest the to but thy godd to wark?\n",
      "\n",
      "BERTRAM:\n",
      "This doubour abble the \n",
      "\n",
      "[41m 50s (1600 32%) train loss: 1.6466, test_loss: 1.7268]\n",
      "What I lovers, Poot it to did them and men and friends and be mattails,\n",
      "So, ficu the the what is a par \n",
      "\n",
      "[42m 45s (1650 33%) train loss: 1.6283, test_loss: 1.7303]\n",
      "Whrid the he drunperst in a with her become, lack thy crear good them and forgets up for's see you the \n",
      "\n",
      "[43m 39s (1700 34%) train loss: 1.6532, test_loss: 1.7340]\n",
      "Why know's bitt; freish san with ode, and provant, we go, to he will migind thee bearter speak to grea \n",
      "\n",
      "[44m 36s (1750 35%) train loss: 1.6378, test_loss: 1.7690]\n",
      "What I have me.\n",
      "\n",
      "DOMISABE:\n",
      "I am not to this\n",
      "Lion.\n",
      "\n",
      "BUCHINCE:\n",
      "You this means poor a mine:\n",
      "Could the a a \n",
      "\n",
      "[45m 30s (1800 36%) train loss: 1.6293, test_loss: 1.7149]\n",
      "What prince for you tres.\n",
      "\n",
      "PRINCE HENRY:\n",
      "That it craved of Duster the deed\n",
      "thee stard never that's you \n",
      "\n",
      "[46m 23s (1850 37%) train loss: 1.6405, test_loss: 1.7575]\n",
      "Where's whophine of eadious, brablation any to it:\n",
      "And be his perter us of thee.\n",
      "\n",
      "FERDINAND:\n",
      "Good thee \n",
      "\n",
      "[47m 16s (1900 38%) train loss: 1.6363, test_loss: 1.7300]\n",
      "What would grace:\n",
      "Why, and the confeir wise;\n",
      "What, lind do preying perfort sumble of Lord up and gothi \n",
      "\n",
      "[48m 8s (1950 39%) train loss: 1.6225, test_loss: 1.7415]\n",
      "Whiser,\n",
      "Which here on the many, I had eyreanity, sir.\n",
      "\n",
      "CRESSINAL\n",
      "O of end.\n",
      "\n",
      "FLUNCENIADO:\n",
      "Stand the Tar \n",
      "\n",
      "[49m 1s (2000 40%) train loss: 1.6375, test_loss: 1.7663]\n",
      "Whilet of a mistreating would I must a buster\n",
      "The cothinks of a sailied discordiness opposot as you sh \n",
      "\n",
      "[49m 54s (2050 41%) train loss: 1.6284, test_loss: 1.7326]\n",
      "Whis to stracques before mother thou, if then how you to hole the thou have drith a pitter'd she with  \n",
      "\n",
      "[50m 47s (2100 42%) train loss: 1.6043, test_loss: 1.7360]\n",
      "WhMBO:\n",
      "Good you uncermie; and at dear man a double distaim as hearts them sailed as the wars with the  \n",
      "\n",
      "[51m 40s (2150 43%) train loss: 1.6657, test_loss: 1.7528]\n",
      "What, and just in thy commthe the king mand's but me and lives.\n",
      "\n",
      "KING CLAUSIO:\n",
      "Were will good wread no \n",
      "\n",
      "[52m 33s (2200 44%) train loss: 1.6288, test_loss: 1.7346]\n",
      "Where so very ears,\n",
      "That is not from a not her good\n",
      "Fore's me the craved for a man and father'd it the \n",
      "\n",
      "[53m 25s (2250 45%) train loss: 1.6235, test_loss: 1.7450]\n",
      "Whrout love,\n",
      "Or fave of the regaint the courted with do and have not?\n",
      "\n",
      "EARL LACDINA:\n",
      "Culding it to thy \n",
      "\n",
      "[54m 18s (2300 46%) train loss: 1.6198, test_loss: 1.7344]\n",
      "What, ans where, and the delive than the rave denity with no soul doth honour diviing to should\n",
      "When w \n",
      "\n",
      "[55m 10s (2350 47%) train loss: 1.6541, test_loss: 1.7274]\n",
      "WhYS:\n",
      "There's that your reports I would be best a man the son my reate and no may be je a purish come  \n",
      "\n",
      "[56m 3s (2400 48%) train loss: 1.6099, test_loss: 1.7474]\n",
      "WhMBARTESS OF YORK:\n",
      "That she shall good her his great.\n",
      "\n",
      "CORNWALS:\n",
      "His renowly the for might is poce an \n",
      "\n",
      "[56m 57s (2450 49%) train loss: 1.6238, test_loss: 1.7372]\n",
      "What of bided,\n",
      "Thy gul will best to did I more?\n",
      "\n",
      "TRANTONINE:\n",
      "It makieve and earth, and you is me hath  \n",
      "\n",
      "[57m 51s (2500 50%) train loss: 1.6232, test_loss: 1.7315]\n",
      "WhMBORISA:\n",
      "O Lord,\n",
      "Do her good in his lies him,\n",
      "And and ne'er princest with thee man are my strace.\n",
      "\n",
      "C \n",
      "\n",
      "[58m 44s (2550 51%) train loss: 1.6354, test_loss: 1.7544]\n",
      "Why counner.\n",
      "\n",
      "Clown:\n",
      "I am you to husband of captenstion and of your heart thee?\n",
      "\n",
      "HAMLET:\n",
      "I with prith  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59m 37s (2600 52%) train loss: 1.6196, test_loss: 1.7363]\n",
      "Why teal thy lords know\n",
      "With my great walk more is her and to touch of you mewe dood with crack breakn \n",
      "\n",
      "[60m 29s (2650 53%) train loss: 1.6396, test_loss: 1.7510]\n",
      "What, sun of resseol the world of a letter be the scait;\n",
      "When to the standly; for flam,\n",
      "Or your way ma \n",
      "\n",
      "[61m 22s (2700 54%) train loss: 1.5923, test_loss: 1.7284]\n",
      "What you actiss your shapess's best\n",
      "chant: contain court\n",
      "But I pray times and gave as so not to purpil \n",
      "\n",
      "[62m 15s (2750 55%) train loss: 1.6224, test_loss: 1.7179]\n",
      "Whrwe hears,\n",
      "The commance. Have waust are or be.\n",
      "\n",
      "SPEERE ARON:\n",
      "If cut at his bustime from me me show,  \n",
      "\n",
      "[63m 7s (2800 56%) train loss: 1.6361, test_loss: 1.7685]\n",
      "What what and you honour made for my prace wop elewlan not't the crow the\n",
      "pads charge the old a house  \n",
      "\n",
      "[64m 0s (2850 56%) train loss: 1.6371, test_loss: 1.7324]\n",
      "Why knave. What not lady?\n",
      "\n",
      "PANDARUS:\n",
      "Not ode and kinning the prove you ladsous weoters us from her nig \n",
      "\n",
      "[65m 9s (2900 57%) train loss: 1.6432, test_loss: 1.7159]\n",
      "Whold\n",
      "dookes's like well you were.\n",
      "\n",
      "PRINCE HENRY:\n",
      "I pran\n",
      "And plaugh of belue did from our might inquic \n",
      "\n",
      "[66m 20s (2950 59%) train loss: 1.6138, test_loss: 1.7194]\n",
      "Why'\n",
      "Inday, it be, and farbius is his never you sufferate mishe\n",
      "parrage, then I am devil, which\n",
      "As in, \n",
      "\n",
      "[67m 36s (3000 60%) train loss: 1.6222, test_loss: 1.7419]\n",
      "Wha may they will deen of strance to it, man the corriet!\n",
      "That your brother, old lian my hand: then th \n",
      "\n",
      "[68m 54s (3050 61%) train loss: 1.6263, test_loss: 1.7081]\n",
      "Whem to John.\n",
      "\n",
      "First to saulies an Eant and not say means to her the forefore is more spirlandience, t \n",
      "\n",
      "[70m 8s (3100 62%) train loss: 1.6505, test_loss: 1.6853]\n",
      "Why, thy send profes more home where I shall confall.\n",
      "\n",
      "CAMILLA:\n",
      "I will should me of Saith\n",
      "Should that  \n",
      "\n",
      "[71m 32s (3150 63%) train loss: 1.6278, test_loss: 1.7130]\n",
      "WhMBY SUBALBRTONE:\n",
      "And them are such these be monstent of theme, thou contolping as our hape still, I  \n",
      "\n",
      "[72m 54s (3200 64%) train loss: 1.6196, test_loss: 1.6663]\n",
      "What he hadl--verted that you are not we good than I\n",
      "prince.\n",
      "\n",
      "PRINCE NDY:\n",
      "Come, his give my blumon a m \n",
      "\n",
      "[74m 14s (3250 65%) train loss: 1.6346, test_loss: 1.7416]\n",
      "What and was drithathed is even fort my huef these Jest fair master that\n",
      "And heard out away\n",
      "the compaz \n",
      "\n",
      "[75m 25s (3300 66%) train loss: 1.6233, test_loss: 1.7152]\n",
      "What be thyseed hear his leave the live my comms my sould:\n",
      "Whose is not sent an own wisure.\n",
      "\n",
      "Fourth\n",
      "in \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(n_characters, hidden_size, n_characters, model_type=model_type, n_layers=n_layers).to(device)\n",
    "rnn_optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "test_losses = []\n",
    "loss_avg = 0\n",
    "test_loss_avg = 0\n",
    "\n",
    "\n",
    "print(\"Training for %d epochs...\" % n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(rnn, *load_random_batch(train_text, chunk_len, batch_size), rnn_optimizer, criterion)\n",
    "    loss_avg += loss\n",
    "    \n",
    "    test_loss = eval_test(rnn, *load_random_batch(test_text, chunk_len, batch_size))\n",
    "    test_loss_avg += test_loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) train loss: %.4f, test_loss: %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, test_loss))\n",
    "        print(generate(rnn, 'Wh', 100, device=device), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        test_losses.append(test_loss_avg / plot_every)\n",
    "        loss_avg = 0\n",
    "        test_loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save network\n",
    "torch.save(rnn.state_dict(), './rnn_generator.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Training and Test Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.plot(test_losses, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate text generation\n",
    "\n",
    "Check what the outputted text looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate(rnn, prime_str='Th', predict_len=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Some things you should try to improve your network performance are:\n",
    "- Different RNN types. Switch the basic RNN network in your model to a GRU and LSTM to compare all three.\n",
    "- Try adding 1 or two more layers\n",
    "- Increase the hidden layer size\n",
    "- Changing the learning rate\n",
    "\n",
    "**TODO:** Try changing the RNN type and hyperparameters. Record your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}