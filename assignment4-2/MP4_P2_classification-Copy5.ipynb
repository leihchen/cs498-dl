{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import glob\n",
    "import string\n",
    "import random \n",
    "# !conda install torch, torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "from rnn.helpers import time_since\n",
    "# !conda install unidecode\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language recognition with an RNN\n",
    "\n",
    "If you've ever used an online translator you've probably seen a feature that automatically detects the input language. While this might be easy to do if you input unicode characters that are unique to one or a small group of languages (like \"你好\" or \"γεια σας\"), this problem is more challenging if the input only uses the available ASCII characters. In this case, something like \"těší mě\" would beome \"tesi me\" in the ascii form. This is a more challenging problem in which the language must be recognized purely by the pattern of characters rather than unique unicode characters.\n",
    "\n",
    "We will train an RNN to solve this problem for a small set of languages thta can be converted to romanized ASCII form. For training data it would be ideal to have a large and varied dataset in different language styles. However, it is easy to find copies of the Bible which is a large text translated to different languages but in the same easily parsable format, so we will use 20 different copies of the Bible as training data. Using the same book for all of the different languages will hopefully prevent minor overfitting that might arise if we used different books for each language (fitting to common characteristics of the individual books rather than the language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tesi me\n"
     ]
    }
   ],
   "source": [
    "from unidecode import unidecode as unicodeToAscii\n",
    "\n",
    "all_characters = string.printable\n",
    "n_letters = len(all_characters)\n",
    "\n",
    "print(unicodeToAscii('těší mě'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a file and split into lines\n",
    "def readFile(filename):\n",
    "    data = open(filename, encoding='utf-8').read().strip()\n",
    "    return unicodeToAscii(data)\n",
    "\n",
    "def get_category_data(data_path):\n",
    "    # Build the category_data dictionary, a list of names per language\n",
    "    category_data = {}\n",
    "    all_categories = []\n",
    "    for filename in glob.glob(data_path):\n",
    "        category = os.path.splitext(os.path.basename(filename))[0].split('_')[0]\n",
    "        all_categories.append(category)\n",
    "        data = readFile(filename)\n",
    "        category_data[category] = data\n",
    "    \n",
    "    return category_data, all_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original text is split into two parts, train and test, so that we can make sure that the model is not simply memorizing the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "['albanian', 'czech', 'danish', 'english', 'esperanto', 'finnish', 'french', 'german', 'hungarian', 'italian', 'lithuanian', 'maori', 'norwegian', 'portuguese', 'romanian', 'spanish', 'swedish', 'turkish', 'vietnamese', 'xhosa']\n"
     ]
    }
   ],
   "source": [
    "train_data_path = 'language_data/train/*_train.txt'\n",
    "test_data_path = 'language_data/test/*_test.txt'\n",
    "\n",
    "train_category_data, all_categories = get_category_data(train_data_path)\n",
    "test_category_data, test_all_categories = get_category_data(test_data_path)\n",
    "\n",
    "n_languages = len(all_categories)\n",
    "\n",
    "print(len(all_categories))\n",
    "print(all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1, dim=1)\n",
    "    category_i = top_i[:, 0]\n",
    "    return category_i\n",
    "\n",
    "# Turn string into long tensor\n",
    "def stringToTensor(string):\n",
    "    tensor = torch.zeros(len(string), requires_grad=True).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor\n",
    "\n",
    "def load_random_batch(text, chunk_len, batch_size):\n",
    "    input_data = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    target = torch.zeros(batch_size, 1).long().to(device)\n",
    "    input_text = []\n",
    "    for i in range(batch_size):\n",
    "        category = all_categories[random.randint(0, len(all_categories) - 1)]\n",
    "        line_start = random.randint(0, len(text[category])-chunk_len)\n",
    "        category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "        line = text[category][line_start:line_start+chunk_len]\n",
    "        input_text.append(line)\n",
    "        input_data[i] = stringToTensor(line)\n",
    "        target[i] = category_tensor\n",
    "    return input_data, target, input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Model\n",
    "====================\n",
    "\n",
    "For this classification task, we can use the same model we implement for the generation task which is located in `rnn/model.py`. See the `MP4_P2_generation.ipynb` notebook for more instructions. In this case each output vector of our RNN will have the dimension of the number of possible languages (i.e. `n_languages`). We will use this vector to predict a distribution over the languages.\n",
    "\n",
    "In the generation task, we used the output of the RNN at every time step to predict the next letter and our loss included the output from each of these predictions. However, in this task we use the output of the RNN at the end of the sequence to predict the language, so our loss function will use only the predicted output from the last time step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn.model import RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_len = 50\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "n_epochs = 7000\n",
    "hidden_size = 160\n",
    "n_layers = 3\n",
    "learning_rate = 0.00015\n",
    "model_type = 'lstm'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "rnn = RNN(n_letters, hidden_size, n_languages, model_type=model_type, n_layers=n_layers).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Fill in the train function. You should initialize a hidden layer representation using your RNN's `init_hidden` function, set the model gradients to zero, and loop over each time step (character) in the input tensor. For each time step compute the output of the of the RNN and the next hidden layer representation. The cross entropy loss should be computed over the last RNN output scores from the end of the sequence and the target classification tensor. Lastly, call backward on the loss and take an optimizer step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, target_tensor, data_tensor, optimizer, criterion, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - rnn: model\n",
    "    - target_target: target character data tensor of shape (batch_size, 1)\n",
    "    - data_tensor: input character data tensor of shape (batch_size, chunk_len)\n",
    "    - optimizer: rnn model optimizer\n",
    "    - criterion: loss function\n",
    "    - batch_size: data batch size\n",
    "    \n",
    "    Returns:\n",
    "    - output: output from RNN from end of sequence \n",
    "    - loss: computed loss value as python float\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    output, loss = None, None\n",
    "    \n",
    "    ####################################\n",
    "    #          YOUR CODE HERE          #\n",
    "    ####################################\n",
    "    hidden = rnn.init_hidden(data_tensor.shape[0])\n",
    "    rnn.zero_grad()\n",
    "    for i in range(chunk_len):\n",
    "        output, hidden = rnn(data_tensor[:, i], hidden)\n",
    "    loss = criterion(output.reshape(batch_size, -1), target_tensor.squeeze())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    ##########       END      ##########\n",
    "\n",
    "    return output, loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(rnn, data_tensor, seq_len=chunk_len, batch_size=BATCH_SIZE):\n",
    "    with torch.no_grad():\n",
    "        data_tensor = data_tensor.to(device)\n",
    "        hidden = rnn.init_hidden(batch_size, device=device)\n",
    "        for i in range(seq_len):\n",
    "            output, hidden = rnn(data_tensor[:,i], hidden)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "def eval_test(rnn, category_tensor, data_tensor):\n",
    "    with torch.no_grad():\n",
    "        output = evaluate(rnn, data_tensor)\n",
    "        loss = criterion(output, category_tensor.squeeze())\n",
    "        return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0% (0m 22s) 2.9817 2.9803 e nje pylli, me trung te gjate dhe me maje qe ngri / english ✗ (albanian)\n",
      "Train accuracy: 0.068\n",
      "100 1% (0m 45s) 2.5805 2.5593 taga i besittning en jordlott bland folket. Nar ha / portuguese ✗ (swedish)\n",
      "Train accuracy: 0.112\n",
      "150 2% (1m 11s) 2.2236 2.1980 ori, Ithnami, Zifi, Telemi, Bealothi, Hatsor-Hadat / lithuanian ✗ (albanian)\n",
      "Train accuracy: 0.2436\n",
      "200 3% (1m 40s) 1.8662 1.7926 ele lui sint cum sint coarnele bivolului; Cu ele v / romanian ✓\n",
      "Train accuracy: 0.3732\n",
      "250 4% (2m 11s) 1.7158 1.6553  vitei grasi si oi in mare numar; si a poftit pe t / italian ✗ (romanian)\n",
      "Train accuracy: 0.4684\n",
      "Test accuracy:  0.404\n",
      "300 5% (3m 55s) 1.3319 1.2912 te mbash qendrim kunder jetes te te afermit tend.  / german ✗ (albanian)\n",
      "Train accuracy: 0.5424\n",
      "350 5% (4m 33s) 1.1866 1.1789 och harlighet; och for den storhets skull som han  / danish ✗ (swedish)\n",
      "Train accuracy: 0.5886\n",
      "400 6% (5m 14s) 1.1802 1.1335 iya hazirlanin.>> Boylece Samiriyeye karsi saldiri / turkish ✓\n",
      "Train accuracy: 0.6384\n",
      "450 7% (6m 0s) 0.9326 0.9005 jes pries pranasa, jis tare: \"Dabar tikrai zinau,  / romanian ✗ (lithuanian)\n",
      "Train accuracy: 0.6624\n",
      "500 8% (6m 51s) 0.9348 0.9282 ao. Den phien cac nguoi Giu-da cung hua theo loi d / vietnamese ✓\n",
      "Train accuracy: 0.6836\n",
      "Test accuracy:  0.532\n",
      "550 9% (7m 58s) 0.6777 0.6436 and var tung over asdoditerna; han anstallde forod / swedish ✓\n",
      "Train accuracy: 0.707\n",
      "600 10% (8m 58s) 0.7470 0.7214 ra Mesja i Retning af Sefar, Ostens Bjerge. Det va / norwegian ✗ (danish)\n",
      "Train accuracy: 0.7312\n",
      "650 10% (10m 4s) 0.9219 0.8466 for grater du? Hvem leter du efter? Hun trodde at  / danish ✗ (norwegian)\n",
      "Train accuracy: 0.7322\n",
      "700 11% (11m 15s) 0.4950 0.4813 t l'Eternel descendit dans la nuee, et se tint la  / french ✓\n",
      "Train accuracy: 0.7484\n",
      "750 12% (12m 33s) 0.6362 0.5820 ea etsimaan mestariasi. Kenties Herran henki on no / finnish ✓\n",
      "Train accuracy: 0.7692\n",
      "Test accuracy:  0.592\n",
      "800 13% (14m 7s) 0.5989 0.5370 a til Orde og sagde: \"I har nu indviet eder til HE / norwegian ✗ (danish)\n",
      "Train accuracy: 0.7634\n",
      "850 14% (15m 36s) 0.7386 0.6976 atott czedrusaidat, es a tuzre vetik. Es sok nep m / lithuanian ✗ (hungarian)\n",
      "Train accuracy: 0.7646\n",
      "900 15% (17m 12s) 0.6447 0.6467 sanoistasi niin kuin riemuitaan suuresta saaliista / finnish ✓\n",
      "Train accuracy: 0.7894\n",
      "950 15% (18m 57s) 0.4777 0.4417  ti istenteknek, Remfannak csillagat, a kepeket, m / hungarian ✓\n",
      "Train accuracy: 0.8024\n",
      "1000 16% (20m 50s) 0.4716 0.4180  ay serbest birakti. Kiz arkadaslariyla birlikte k / turkish ✓\n",
      "Train accuracy: 0.8026\n",
      "Test accuracy:  0.616\n",
      "1050 17% (23m 2s) 0.5455 0.4946 . Rauha ja menestys Israelille! Matkalaulu.  Kovin / finnish ✓\n",
      "Train accuracy: 0.8144\n",
      "1100 18% (25m 11s) 0.5415 0.5320 raon, si Faraon n'a vrut sa -i lase sa plece. Fara / esperanto ✗ (romanian)\n",
      "Train accuracy: 0.8166\n",
      "1150 19% (27m 30s) 0.5208 0.5379 ut, sa den ene: Fly for ditt livs skyld, se dig ik / norwegian ✓\n",
      "Train accuracy: 0.8346\n",
      "1200 20% (29m 52s) 0.3981 0.3719 eblas, a la luz seran oidas; y lo que hablasteis a / spanish ✓\n",
      "Train accuracy: 0.8226\n",
      "1250 20% (32m 18s) 0.4113 0.4301 rdens port skall goras ett forhange, tjugu alnar l / swedish ✓\n",
      "Train accuracy: 0.838\n",
      "Test accuracy:  0.653\n",
      "1300 21% (34m 56s) 0.4031 0.4282 iscxiploj trans la torenton Kidron, kie estis gxar / esperanto ✓\n",
      "Train accuracy: 0.841\n",
      "1350 22% (37m 22s) 0.3805 0.3331 omme, han skal bortrydde ugudelighet fra Jakob, og / norwegian ✓\n",
      "Train accuracy: 0.8452\n",
      "1400 23% (39m 52s) 0.5250 0.4680 s do que todas as coisas, e perverso; quem o poder / portuguese ✓\n",
      "Train accuracy: 0.8482\n",
      "1450 24% (42m 26s) 0.3655 0.3144 ine getirir. Nerede iki ya da uc kisi benim adimla / turkish ✓\n",
      "Train accuracy: 0.854\n",
      "1500 25% (45m 3s) 0.3506 0.3340 o babengamagorha anobukroti. Kwaphuma kwaza kulwa  / xhosa ✓\n",
      "Train accuracy: 0.8482\n",
      "Test accuracy:  0.668\n",
      "1550 25% (47m 57s) 0.4380 0.4175 aj cxiuj popoloj militis kontraux Jerusalem kaj ko / esperanto ✓\n",
      "Train accuracy: 0.8734\n",
      "1600 26% (50m 44s) 0.2813 0.2264 waren diese: Alwan, Manabath, Ebal, Sepho und Onam / english ✗ (german)\n",
      "Train accuracy: 0.8802\n",
      "1650 27% (53m 34s) 0.3425 0.3565 h forutom alla frivilliga offer som I given at HER / swedish ✓\n",
      "Train accuracy: 0.8632\n",
      "1700 28% (56m 30s) 0.3055 0.2896 ian pravecon, Tiam Li indulgas lin, kaj diras:   L / esperanto ✓\n",
      "Train accuracy: 0.8764\n",
      "1750 29% (59m 30s) 0.4418 0.4436 u, til disse dine Byer! Hvor laenge vil du dog tov / danish ✓\n",
      "Train accuracy: 0.8774\n",
      "Test accuracy:  0.686\n",
      "1800 30% (62m 44s) 0.2872 0.2979 der pa iver, mine sonner! For eder har Herren utva / danish ✗ (norwegian)\n",
      "Train accuracy: 0.8856\n",
      "1850 30% (65m 50s) 0.3349 0.3388 , fende l'aria, come l'aquila, spiega le sue ali v / italian ✓\n",
      "Train accuracy: 0.8888\n",
      "1900 31% (69m 1s) 0.3013 0.3083  sutasi, si a alergat la ei. Cind au vazut pe capi / romanian ✓\n",
      "Train accuracy: 0.9\n",
      "1950 32% (72m 20s) 0.2992 0.3131 sunu soyleyeyim, bir hardal tanesi kadar imaniniz  / turkish ✓\n",
      "Train accuracy: 0.8852\n",
      "2000 33% (75m 46s) 0.4796 0.3661 ierra? Y Jehova dijo a Moises: Tambien hare esto q / spanish ✓\n",
      "Train accuracy: 0.8882\n",
      "Test accuracy:  0.675\n",
      "2050 34% (79m 30s) 0.2884 0.2689 and il sera venu, il nous annoncera toutes choses. / french ✓\n",
      "Train accuracy: 0.8862\n",
      "2100 35% (83m 5s) 0.3242 0.2780 it. Me siguri do te terhiqen edhe me te vegjlit e  / albanian ✓\n",
      "Train accuracy: 0.8962\n",
      "2150 35% (86m 46s) 0.2543 0.2580 rezultatus. Izraelyje buvo astuoni simtai tukstanc / lithuanian ✓\n",
      "Train accuracy: 0.897\n",
      "2200 36% (90m 28s) 0.2120 0.2096  Lahmam, and Chitlish, and Gederoth, Beth-dagon, a / english ✓\n",
      "Train accuracy: 0.9056\n",
      "2250 37% (94m 17s) 0.2948 0.2648 ken gecinceye dek saklasan,<br />Bana bir sure ver / turkish ✓\n",
      "Train accuracy: 0.8988\n",
      "Test accuracy:  0.724\n",
      "2300 38% (98m 24s) 0.2472 0.2129 euch den Priestern! Und es begab sich, wahrend sie / german ✓\n",
      "Train accuracy: 0.9038\n",
      "2350 39% (102m 23s) 0.2382 0.2549 nil. Doterne mouchy vnikly do domu faraonova, do d / czech ✓\n",
      "Train accuracy: 0.899\n",
      "2400 40% (106m 25s) 0.3641 0.3946 o. Io ho suscitato Ciro, nella mia giustizia, e ap / portuguese ✗ (italian)\n",
      "Train accuracy: 0.902\n",
      "2450 40% (110m 30s) 0.2972 0.2456 ano ahau ki a koutou ki te whakawa; ka hohoro ano  / maori ✓\n",
      "Train accuracy: 0.8996\n",
      "2500 41% (114m 40s) 0.3092 0.2569 ombi, bazibona oonyana bakaThixo iintombi zabantu  / xhosa ✓\n",
      "Train accuracy: 0.9198\n",
      "Test accuracy:  0.735\n",
      "2550 42% (119m 6s) 0.3222 0.2941 is Dovydo namu ir gimines. Jis ejo uzsirasyti kart / lithuanian ✓\n",
      "Train accuracy: 0.9172\n",
      "Test accuracy:  0.746\n",
      "2600 43% (123m 38s) 0.2212 0.2165  ha mandati da me: Cosi dice l'Eterno: Ecco, io fa / italian ✓\n",
      "Train accuracy: 0.9172\n",
      "Test accuracy:  0.76\n",
      "2650 44% (128m 16s) 0.2187 0.2182 es, tendreis en abominacion. Y por estas cosas ser / spanish ✓\n",
      "Train accuracy: 0.9194\n",
      "Test accuracy:  0.73\n",
      "2700 45% (132m 58s) 0.2755 0.2843 zur, prophete de Gabaon, me parla dans la maison d / french ✓\n",
      "Train accuracy: 0.9214\n",
      "Test accuracy:  0.716\n",
      "2750 45% (137m 43s) 0.2320 0.2114  sta op, tag Saede, Jerusalem, fri dig for Halslae / norwegian ✗ (danish)\n",
      "Train accuracy: 0.9144\n",
      "Test accuracy:  0.737\n",
      "2800 46% (142m 35s) 0.2166 0.2261 nes der atter og to af hans Disciple. Og idet han  / danish ✓\n",
      "Train accuracy: 0.9192\n",
      "Test accuracy:  0.739\n",
      "2850 47% (147m 30s) 0.1834 0.1180  gonderdi. Ates sunagin uzerindeki yakmalik sunuyu / turkish ✓\n",
      "Train accuracy: 0.9212\n",
      "Test accuracy:  0.721\n",
      "2900 48% (152m 31s) 0.3529 0.3235 ue o sol ja se havia posto; e, tomando uma das ped / portuguese ✓\n",
      "Train accuracy: 0.9254\n",
      "Test accuracy:  0.757\n",
      "2950 49% (157m 36s) 0.1534 0.1284 e whakatakoto kupu tinihanga ratou mo te hunga ata / xhosa ✗ (maori)\n",
      "Train accuracy: 0.917\n",
      "Test accuracy:  0.719\n",
      "3000 50% (162m 44s) 0.1830 0.2049 leleknek gyotrelme! Gyuloltem en minden munkamat i / hungarian ✓\n",
      "Train accuracy: 0.917\n",
      "Test accuracy:  0.739\n",
      "3050 50% (168m 0s) 0.1614 0.1633 alade till Mose och sade: Detta ar det offer som A / swedish ✓\n",
      "Train accuracy: 0.9192\n",
      "Test accuracy:  0.766\n",
      "3100 51% (173m 22s) 0.1898 0.2009 umbas asgje nga te gjitha ato qe ai me ka dhene, p / albanian ✓\n",
      "Train accuracy: 0.9246\n",
      "Test accuracy:  0.772\n",
      "3150 52% (178m 51s) 0.2413 0.2405  after you, to hold for a possession; of them shal / english ✓\n",
      "Train accuracy: 0.9224\n",
      "Test accuracy:  0.733\n",
      "3200 53% (184m 21s) 0.2292 0.1936 inliyor,<br />Erden kizlari sikintida, kendisi de  / turkish ✓\n",
      "Train accuracy: 0.9316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.73\n",
      "3250 54% (189m 56s) 0.1918 0.1957 ' munn skrev disse ord i en bok, i Judas konge Joj / norwegian ✓\n",
      "Train accuracy: 0.927\n",
      "Test accuracy:  0.756\n",
      "3300 55% (195m 32s) 0.2301 0.2305 stredni nocni hlidky, prave kdyz stridali straze.  / czech ✓\n",
      "Train accuracy: 0.9182\n",
      "Test accuracy:  0.728\n",
      "3350 55% (201m 12s) 0.2743 0.2364  nel mio santuario, a vostro pro. Cosi parla il Si / italian ✓\n",
      "Train accuracy: 0.9284\n",
      "Test accuracy:  0.76\n",
      "3400 56% (207m 1s) 0.3218 0.3177 a portas felsakon da vino. Kaj ili salutos vin, ka / esperanto ✓\n",
      "Train accuracy: 0.9326\n",
      "Test accuracy:  0.74\n",
      "3450 57% (212m 55s) 0.1591 0.1857  bon sao luc to bieu ma Tat-te-nai, quan tong doc  / vietnamese ✓\n",
      "Train accuracy: 0.937\n",
      "Test accuracy:  0.759\n",
      "3500 58% (218m 50s) 0.3151 0.2717 oc linga Mine; vei sta pe stinca. Si cind va trece / romanian ✓\n",
      "Train accuracy: 0.9316\n",
      "Test accuracy:  0.777\n",
      "3550 59% (224m 49s) 0.3288 0.2914 der, och talen sa till honom: Sa sager HERREN: Ar  / swedish ✓\n",
      "Train accuracy: 0.929\n",
      "Test accuracy:  0.769\n",
      "3600 60% (230m 54s) 0.2890 0.2404 jestlize zpet, tez ho nepostrehnu, jestlize neco u / czech ✓\n",
      "Train accuracy: 0.9366\n",
      "Test accuracy:  0.745\n",
      "3650 60% (237m 3s) 0.1589 0.1547 sun!>> Butun halk, <<Amin!>> diyerek RABbe ovguler / turkish ✓\n",
      "Train accuracy: 0.9274\n",
      "Test accuracy:  0.749\n",
      "3700 61% (243m 12s) 0.3011 0.3115 e oraculo. Nao te alegres, o Filistia toda, por se / portuguese ✓\n",
      "Train accuracy: 0.9352\n",
      "Test accuracy:  0.759\n",
      "3750 62% (249m 28s) 0.2216 0.1967 esve te tij, pas tij, kur ai ende s'kishte asnje f / albanian ✓\n",
      "Train accuracy: 0.9432\n",
      "Test accuracy:  0.768\n",
      "3800 63% (255m 52s) 0.0821 0.0950 nga kubekho singangaso ezweni lonke lamaYiputa, ku / xhosa ✓\n",
      "Train accuracy: 0.9436\n",
      "Test accuracy:  0.765\n",
      "3850 64% (262m 20s) 0.1066 0.0737 te aspru pe cei mindri. Fiti tari, si imbarbatati- / romanian ✓\n",
      "Train accuracy: 0.9428\n",
      "Test accuracy:  0.738\n",
      "3900 65% (268m 52s) 0.1648 0.1392 y, i veshur shkelqyeshem, qe ecen ne madheshtine e / albanian ✓\n",
      "Train accuracy: 0.9408\n",
      "Test accuracy:  0.761\n",
      "3950 65% (275m 29s) 0.2547 0.2237 lmasin> dedi. Adamlarima gelince, belli bir yere g / turkish ✓\n",
      "Train accuracy: 0.945\n",
      "Test accuracy:  0.784\n",
      "4000 66% (282m 9s) 0.2059 0.1967  eterne. Jen estas la lastaj vortoj de David:   Pa / esperanto ✓\n",
      "Train accuracy: 0.9358\n",
      "Test accuracy:  0.753\n",
      "4050 67% (288m 53s) 0.2142 0.1929  dirai ouvertement: Je ne vous ai jamais connus; r / french ✓\n",
      "Train accuracy: 0.945\n",
      "Test accuracy:  0.769\n",
      "4100 68% (295m 41s) 0.2229 0.1746 nittiosex utat; men tillsammans voro granatapplena / esperanto ✗ (swedish)\n",
      "Train accuracy: 0.9382\n",
      "Test accuracy:  0.768\n",
      "4150 69% (302m 33s) 0.1551 0.1420 in, ja nyt on minulle karttunut vakea ja karjaa ka / finnish ✓\n",
      "Train accuracy: 0.9448\n",
      "Test accuracy:  0.758\n",
      "4200 70% (309m 30s) 0.2225 0.2097 aroi vera, nena e Jezusit i tha: ''Nuk kane me ver / albanian ✓\n",
      "Train accuracy: 0.9432\n",
      "Test accuracy:  0.776\n",
      "4250 70% (316m 31s) 0.2428 0.1949 ahettanyt minut, niin lahetan mina teidat.\" Sanott / finnish ✓\n",
      "Train accuracy: 0.9532\n",
      "Test accuracy:  0.767\n",
      "4300 71% (323m 39s) 0.1005 0.1134  por cxiu pordego aparte. Kaj la loto pri la orien / esperanto ✓\n",
      "Train accuracy: 0.9382\n",
      "Test accuracy:  0.759\n",
      "4350 72% (330m 51s) 0.2005 0.2040 imo, tas bukaprotis. Geras zmogus susilaukia Viesp / lithuanian ✓\n",
      "Train accuracy: 0.9388\n",
      "Test accuracy:  0.795\n",
      "4400 73% (338m 8s) 0.2073 0.2139 bildirilirse, duyduklarinizi iyice arastirin. Duyd / turkish ✓\n",
      "Train accuracy: 0.948\n",
      "Test accuracy:  0.75\n",
      "4450 74% (345m 30s) 0.1256 0.1167 oku kanohi, ka kite, na ko nga wahine tokorua e pu / maori ✓\n",
      "Train accuracy: 0.9508\n",
      "Test accuracy:  0.776\n",
      "4500 75% (352m 56s) 0.1692 0.1465 dekuren tende\". Atehere Abrahami u ngrit, u perul  / albanian ✓\n",
      "Train accuracy: 0.9468\n",
      "Test accuracy:  0.772\n",
      "4550 75% (360m 27s) 0.1512 0.1257 u; Chung toi di voi doan dong den nha Duc Chua Tro / vietnamese ✓\n",
      "Train accuracy: 0.9494\n",
      "Test accuracy:  0.787\n",
      "4600 76% (368m 4s) 0.1758 0.1734  zonke, izikhulu zakwaSirayeli eziziintloko zeendl / xhosa ✓\n",
      "Train accuracy: 0.9474\n",
      "Test accuracy:  0.762\n",
      "4650 77% (375m 42s) 0.1365 0.1170 a incuiat usa. Mai pe urma, au venit si celelalte  / romanian ✓\n",
      "Train accuracy: 0.9508\n",
      "Test accuracy:  0.769\n",
      "4700 78% (383m 22s) 0.1571 0.1056 o Hamuera. Na ka mea ratou, Kahore; engari me whai / maori ✓\n",
      "Train accuracy: 0.9514\n",
      "Test accuracy:  0.77\n",
      "4750 79% (391m 6s) 0.1584 0.1387  para que lo cumplamos? Ni esta de la otra parte d / spanish ✓\n",
      "Train accuracy: 0.9498\n",
      "Test accuracy:  0.779\n",
      "4800 80% (398m 58s) 0.1341 0.1160 restis cxe Bileam. Kaj Dio venis al Bileam, kaj di / esperanto ✓\n",
      "Train accuracy: 0.9544\n",
      "Test accuracy:  0.809\n",
      "4850 80% (406m 53s) 0.1715 0.1024 ini yemeyin, ona dokunmayin; yoksa olursunuz> dedi / turkish ✓\n",
      "Train accuracy: 0.9472\n",
      "Test accuracy:  0.795\n",
      "4900 81% (414m 57s) 0.1692 0.1310 sekseen: \"Ei Herra tee hyvaa eika pahaa.\" Heidan o / finnish ✓\n",
      "Train accuracy: 0.9544\n",
      "Test accuracy:  0.809\n",
      "4950 82% (423m 2s) 0.1433 0.1543  izigodo zooyise. Nam ndabamisela imimiselo engaba / xhosa ✓\n",
      "Train accuracy: 0.9556\n",
      "Test accuracy:  0.788\n",
      "5000 83% (431m 12s) 0.1422 0.1038 che mostravano grande intelligenza nel servizio de / italian ✓\n",
      "Train accuracy: 0.9518\n",
      "Test accuracy:  0.768\n",
      "5050 84% (439m 25s) 0.1242 0.1052 a,  nimbethelele emnqamlezweni; kuba mna andifuman / xhosa ✓\n",
      "Train accuracy: 0.949\n",
      "Test accuracy:  0.787\n",
      "5100 85% (447m 48s) 0.0519 0.0675 ><<Burada oturacagim, cunku bunu kendim istedim. C / turkish ✓\n",
      "Train accuracy: 0.9516\n",
      "Test accuracy:  0.795\n",
      "5150 85% (456m 12s) 0.1430 0.1913  ne dusunduklerini bilen Isa dedi ki, <<Yureginizd / turkish ✓\n",
      "Train accuracy: 0.9584\n",
      "Test accuracy:  0.784\n",
      "5200 86% (464m 41s) 0.0813 0.0629  chrame zide z provincie Asie. Pobourili cely dav, / czech ✓\n",
      "Train accuracy: 0.9544\n",
      "Test accuracy:  0.789\n",
      "5250 87% (473m 13s) 0.0920 0.0931  paita hienosta pellavasta, tee turbaani hienosta  / finnish ✓\n",
      "Train accuracy: 0.953\n",
      "Test accuracy:  0.772\n",
      "5300 88% (481m 48s) 0.1832 0.1256 erra de Jehova, sino que volvera Ephraim a Egipto, / portuguese ✗ (spanish)\n",
      "Train accuracy: 0.9546\n",
      "Test accuracy:  0.792\n",
      "5350 89% (490m 30s) 0.2300 0.2175 shlukuji se na nebi vody, privadi mlhu od koncin z / czech ✓\n",
      "Train accuracy: 0.9552\n",
      "Test accuracy:  0.801\n",
      "5400 90% (499m 15s) 0.0906 0.0663 ka haere atu ia ki a Ahapa, ki Hamaria. A he tini  / maori ✓\n",
      "Train accuracy: 0.9576\n",
      "Test accuracy:  0.805\n",
      "5450 90% (508m 7s) 0.1803 0.1528 hez hasonlo, es megerosite engem. Es monda: Ne fel / hungarian ✓\n",
      "Train accuracy: 0.9524\n",
      "Test accuracy:  0.806\n",
      "5500 91% (516m 58s) 0.0936 0.0563  Y-so-ra-en. Da-vit hoi nhung nguoi o gan minh ran / vietnamese ✓\n",
      "Train accuracy: 0.9564\n",
      "Test accuracy:  0.772\n",
      "5550 92% (526m 0s) 0.1513 0.1375 ue Izraelcum: \"Jak dlouho budete otalet, nez konec / czech ✓\n",
      "Train accuracy: 0.9566\n",
      "Test accuracy:  0.818\n",
      "5600 93% (535m 5s) 0.1533 0.1296 ek elottem megepedne, es a leheletek, a kiket en t / hungarian ✓\n",
      "Train accuracy: 0.956\n",
      "Test accuracy:  0.8\n",
      "5650 94% (544m 17s) 0.1627 0.1147 rti i tij dritherohet. Zemra ime vajton per Moabin / albanian ✓\n",
      "Train accuracy: 0.9536\n",
      "Test accuracy:  0.775\n",
      "5700 95% (553m 33s) 0.0674 0.0756  thee, three days' journey into the wilderness, th / english ✓\n",
      "Train accuracy: 0.9602\n",
      "Test accuracy:  0.811\n",
      "5750 95% (562m 49s) 0.1864 0.1363 eruya, nabakhonzi bakaDavide,  baqubisana nabo ech / xhosa ✓\n",
      "Train accuracy: 0.9596\n",
      "Test accuracy:  0.804\n",
      "5800 96% (572m 14s) 0.0714 0.0619 day to ton kinh chu minh. Vay neu ta la cha, nao s / vietnamese ✓\n",
      "Train accuracy: 0.9634\n",
      "Test accuracy:  0.799\n"
     ]
    }
   ],
   "source": [
    "n_iters = 6000 #100000\n",
    "print_every = 50\n",
    "plot_every = 50\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "current_test_loss = 0\n",
    "all_losses = []\n",
    "all_test_losses = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "number_correct = 0\n",
    "for iter in range(1, n_iters + 1):\n",
    "    input_data, target_category, text_data = load_random_batch(train_category_data, chunk_len, BATCH_SIZE)\n",
    "    output, loss = train(rnn, target_category, input_data, optimizer, criterion)\n",
    "    current_loss += loss\n",
    "    \n",
    "    _, test_loss = eval_test(rnn, target_category, input_data)\n",
    "    current_test_loss += test_loss\n",
    "    \n",
    "    guess_i = categoryFromOutput(output)\n",
    "    number_correct += (target_category.squeeze()==guess_i.squeeze()).long().sum()\n",
    "    \n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        sample_idx = 0\n",
    "        guess = all_categories[guess_i[sample_idx]]\n",
    "        \n",
    "        category = all_categories[int(target_category[sample_idx])]\n",
    "        \n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %.4f %s / %s %s' % (iter, iter / n_iters * 100, time_since(start), loss, test_loss, text_data[sample_idx], guess, correct))\n",
    "        print('Train accuracy: {}'.format(float(number_correct)/float(print_every*BATCH_SIZE)))\n",
    "        number_correct = 0\n",
    "    \n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0\n",
    "        all_test_losses.append(current_test_loss / plot_every)\n",
    "        current_test_loss = 0\n",
    "    if iter % (print_every*5) == 0 or (iter > 2500 and iter % (print_every) == 0):\n",
    "        eval_batch_size = 1  # needs to be set to 1 for evaluating different sequence lengths\n",
    "\n",
    "        # Keep track of correct guesses in a confusion matrix\n",
    "        confusion = torch.zeros(n_languages, n_languages)\n",
    "        n_confusion = 1000\n",
    "        num_correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for i in range(n_confusion):\n",
    "            eval_chunk_len = random.randint(10, 50) # in evaluation we will look at sequences of variable sizes\n",
    "            input_data, target_category, text_data = load_random_batch(test_category_data, chunk_len=eval_chunk_len, batch_size=eval_batch_size)\n",
    "            output = evaluate(rnn, input_data, seq_len=eval_chunk_len, batch_size=eval_batch_size)\n",
    "\n",
    "            guess_i = categoryFromOutput(output)\n",
    "            category_i = [int(target_category[idx]) for idx in range(len(target_category))]\n",
    "            for j in range(eval_batch_size):\n",
    "                category = all_categories[category_i[j]] \n",
    "                confusion[category_i[j]][guess_i[j]] += 1\n",
    "                num_correct += int(guess_i[j]==category_i[j])\n",
    "                total += 1\n",
    "\n",
    "        print('Test accuracy: ', float(num_correct)/float(n_confusion*eval_batch_size))\n",
    "        if float(num_correct)/float(n_confusion*eval_batch_size) > .89 or iter == 250:\n",
    "            torch.save(rnn.state_dict(), './rnn_classifier'+str(iter)+'.pth')\n",
    "            kaggle_test_file_path = 'language_data/kaggle_rnn_language_classification_test.txt'\n",
    "            with open(kaggle_test_file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            output_rows = []\n",
    "            for i, line in enumerate(lines):\n",
    "                sample = line.rstrip()\n",
    "                sample_chunk_len = len(sample)\n",
    "                input_data = stringToTensor(sample).unsqueeze(0)\n",
    "                output = evaluate(rnn, input_data, seq_len=sample_chunk_len, batch_size=1)\n",
    "                guess_i = categoryFromOutput(output)\n",
    "                output_rows.append((str(i+1), all_categories[guess_i]))\n",
    "\n",
    "            submission_file_path = 'kaggle_rnn_submission'+str(iter)+'.txt'\n",
    "            with open(submission_file_path, 'w') as f:\n",
    "                output_rows = [('id', 'category')] + output_rows\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerows(output_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss functions\n",
    "--------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses, color='b')\n",
    "plt.plot(all_test_losses, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate results\n",
    "-------------------\n",
    "\n",
    "We now vizualize the performance of our model by creating a confusion matrix. The ground truth languages of samples are represented by rows in the matrix while the predicted languages are represented by columns.\n",
    "\n",
    "In this evaluation we consider sequences of variable sizes rather than the fixed length sequences we used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch_size = 1  # needs to be set to 1 for evaluating different sequence lengths\n",
    "\n",
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_languages, n_languages)\n",
    "n_confusion = 1000\n",
    "num_correct = 0\n",
    "total = 0\n",
    "\n",
    "for i in range(n_confusion):\n",
    "    eval_chunk_len = random.randint(10, 50) # in evaluation we will look at sequences of variable sizes\n",
    "    input_data, target_category, text_data = load_random_batch(test_category_data, chunk_len=eval_chunk_len, batch_size=eval_batch_size)\n",
    "    output = evaluate(rnn, input_data, seq_len=eval_chunk_len, batch_size=eval_batch_size)\n",
    "    \n",
    "    guess_i = categoryFromOutput(output)\n",
    "    category_i = [int(target_category[idx]) for idx in range(len(target_category))]\n",
    "    for j in range(eval_batch_size):\n",
    "        category = all_categories[category_i[j]] \n",
    "        confusion[category_i[j]][guess_i[j]] += 1\n",
    "        num_correct += int(guess_i[j]==category_i[j])\n",
    "        total += 1\n",
    "\n",
    "print('Test accuracy: ', float(num_correct)/float(n_confusion*eval_batch_size))\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_languages):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pick out bright spots off the main axis that show which\n",
    "languages it guesses incorrectly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on User Input\n",
    "---------------------\n",
    "\n",
    "Now you can test your model on your own input. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_line, n_predictions=5):\n",
    "    print('\\n> %s' % input_line)\n",
    "    with torch.no_grad():\n",
    "        input_data = stringToTensor(input_line).long().unsqueeze(0).to(device)\n",
    "        output = evaluate(rnn, input_data, seq_len=len(input_line), batch_size=1)\n",
    "\n",
    "    # Get top N categories\n",
    "    topv, topi = output.topk(n_predictions, dim=1)\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(n_predictions):\n",
    "        topv.shape\n",
    "        topi.shape\n",
    "        value = topv[0][i].item()\n",
    "        category_index = topi[0][i].item()\n",
    "        print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "        predictions.append([value, all_categories[category_index]])\n",
    "\n",
    "predict('This is a phrase to test the model on user input')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Kaggle submission file\n",
    "\n",
    "Once you have found a good set of hyperparameters submit the output of your model on the Kaggle test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT CHANGE KAGGLE SUBMISSION CODE ####\n",
    "import csv\n",
    "\n",
    "kaggle_test_file_path = 'language_data/kaggle_rnn_language_classification_test.txt'\n",
    "with open(kaggle_test_file_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "output_rows = []\n",
    "for i, line in enumerate(lines):\n",
    "    sample = line.rstrip()\n",
    "    sample_chunk_len = len(sample)\n",
    "    input_data = stringToTensor(sample).unsqueeze(0)\n",
    "    output = evaluate(rnn, input_data, seq_len=sample_chunk_len, batch_size=1)\n",
    "    guess_i = categoryFromOutput(output)\n",
    "    output_rows.append((str(i+1), all_categories[guess_i]))\n",
    "\n",
    "submission_file_path = 'kaggle_rnn_submission.txt'\n",
    "with open(submission_file_path, 'w') as f:\n",
    "    output_rows = [('id', 'category')] + output_rows\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(output_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
